{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar100_fine-15*15-128.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM27VeymW62Cw19w+8E/ob2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehang/Improvised-CNN/blob/master/notebook/Cifar-100-128-fine/resized/15*15/cifar100_fine_15_15_128.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yidYo4zCxGcb",
        "outputId": "12dfbe16-2e53-4998-deeb-a4492ebaf6c9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLIPBW-yxxwB"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import pathlib\n",
        "import time\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9oqfW4wx5tM"
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/Mehang\\ Rai/cifar100_128_fine.zip -d cifar100_128_fine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLrDbwA8yCb2"
      },
      "source": [
        "ITERATION = 1\n",
        "IMAGE_WIDTH=128\n",
        "IMAGE_HEIGHT=128\n",
        "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "IMAGE_CHANNELS=3\n",
        "RANDOM_SEED = [42, 42, 57, 48, 86, 7, 15, 28, 39, 52][ITERATION-1]\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 100\n",
        "EPOCHS = 1000\n",
        "GABOR_LAYER_INDEX = 0\n",
        "GABOR_WIDTH = 15\n",
        "GABOR_HEIGHT = 15\n",
        "GABOR_SIZE = (GABOR_WIDTH, GABOR_HEIGHT)\n",
        "NUM_RECEPTIVE_FILTERS = 32\n",
        "TRAIN_DIR = \"cifar100_128_fine/cifar100_128_fine/train/\"\n",
        "TEST_DIR = \"cifar100_128_fine/cifar100_128_fine/test/\"\n",
        "# TEST_DIR = \"cifar100_128_fine/cifar100_128_fine/train/\"\n",
        "# TRAIN_DIR = \"cifar100_128_fine/cifar100_128_fine/test/\""
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh6_BZuBzmCE"
      },
      "source": [
        "filenames = os.listdir(TRAIN_DIR)\n",
        "categories = []\n",
        "for filename in filenames:\n",
        "    category = filename.split('.')[0]\n",
        "    categories.append(category)\n",
        "\n",
        "train_df = pd.DataFrame({\n",
        "    'filename': filenames,\n",
        "    'category': categories\n",
        "})\n",
        "\n",
        "filenames = os.listdir(TEST_DIR)\n",
        "categories = []\n",
        "for filename in filenames:\n",
        "    category = filename.split('.')[0]\n",
        "    categories.append(category)\n",
        "\n",
        "validate_df = pd.DataFrame({\n",
        "    'filename': filenames,\n",
        "    'category': categories\n",
        "})"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiNt-8InznNT"
      },
      "source": [
        "train_df = train_df.reset_index(drop=True)\n",
        "validate_df = validate_df.reset_index(drop=True)\n",
        "total_train = train_df.shape[0]\n",
        "total_validate = validate_df.shape[0]"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "OfXfZisAz1FG",
        "outputId": "51b179b7-aa18-4fbb-9daf-b3369bd282f7"
      },
      "source": [
        "train_df['category'].value_counts().plot.bar()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f63773499b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "_CXsiVhfz2_P",
        "outputId": "be02b494-c403-41ce-edd7-71b0277d9492"
      },
      "source": [
        "validate_df['category'].value_counts().plot.bar()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f64330cec88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PIJxj2wz47t",
        "outputId": "a96a6bea-2e57-41a0-8fa6-8ab8443d1de5"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    # rotation_range=40,\n",
        "    # width_shift_range=0.2,\n",
        "    # height_shift_range=0.2,\n",
        "    # shear_range=0.2,\n",
        "    # zoom_range=0.2,\n",
        "    # horizontal_flip=True,\n",
        "    rescale=1./255,\n",
        ")\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df, \n",
        "    TRAIN_DIR, \n",
        "    x_col='filename',\n",
        "    y_col='category',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    class_mode='categorical',\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 50000 validated image filenames belonging to 100 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmZsnb4Qz_c0",
        "outputId": "447325f2-bdf4-4320-fcc6-282e462f55d1"
      },
      "source": [
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_generator = validation_datagen.flow_from_dataframe(\n",
        "    validate_df, \n",
        "    TEST_DIR, \n",
        "    x_col='filename',\n",
        "    y_col='category',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    class_mode='categorical',\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 validated image filenames belonging to 100 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubu8-wWQ0Ba9",
        "outputId": "23714d37-0439-4204-cff2-cea8cc0f3e67"
      },
      "source": [
        "print(train_df.shape)\n",
        "print(validate_df.shape)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 2)\n",
            "(10000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-D7iNGo0U5M",
        "outputId": "9448d4fa-1ef9-4f7e-fbd1-6489a7747f06"
      },
      "source": [
        "train_generator.image_shape"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 128, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cipIxpVg0bfp",
        "outputId": "34fbf9f7-dc40-4736-bac9-522d984da99a"
      },
      "source": [
        "NUM_CLASSES = len(train_df['category'].value_counts())\n",
        "print(NUM_CLASSES)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EyIOjr9PKkv",
        "outputId": "b99da92b-d077-483c-ffb3-b61612f43632",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Importing the Keras libraries and packages\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Activation\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "classifier = None\n",
        "classifier = Sequential([\n",
        "    layers.Conv2D(NUM_RECEPTIVE_FILTERS, kernel_size=GABOR_SIZE, strides=(1,1), padding='same', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.5),\n",
        "    layers.Conv2D(128, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.5),\n",
        "    layers.Conv2D(256, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.5),\n",
        "    layers.Conv2D(512, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.5),\n",
        "    layers.Conv2D(512, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    layers.Flatten(),\n",
        "    Dropout(0.5),\n",
        "    layers.Dense(4096),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    layers.Dense(4096),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "classifier.summary()\n",
        "\n",
        "import copy\n",
        "untrained_layers = copy.deepcopy([classifier.get_layer(name=classifier.layers[GABOR_LAYER_INDEX].name).get_weights(),\n",
        "                                  classifier.get_layer(name=classifier.layers[GABOR_LAYER_INDEX+1].name).get_weights()])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,  \n",
        "                              min_delta=1e-4, mode='min', verbose=1)\n",
        "stop_alg = EarlyStopping(monitor='val_loss', patience=35, \n",
        "                         restore_best_weights=True, verbose=1)\n",
        "callbacks = [stop_alg, reduce_lr]\n",
        "opt = Adam(learning_rate=0.001)\n",
        "classifier.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', 'AUC'])\n",
        "\n",
        "start = time.perf_counter()\n",
        "hist = classifier.fit(\n",
        "    train_generator, \n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=total_validate//BATCH_SIZE,\n",
        "    steps_per_epoch=total_train//BATCH_SIZE,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "finish = time.perf_counter()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 128, 128, 32)      21632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 64, 64, 128)       36992     \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 64, 64, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 32, 32, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 32, 32, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 16, 16, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 8, 8, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 4096)              33558528  \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               409700    \n",
            "=================================================================\n",
            "Total params: 54,681,828\n",
            "Trainable params: 54,662,564\n",
            "Non-trainable params: 19,264\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "1562/1562 [==============================] - 64s 40ms/step - loss: 6.2535 - accuracy: 0.0363 - auc: 0.6032 - val_loss: 6.2726 - val_accuracy: 0.0742 - val_auc: 0.7039\n",
            "Epoch 2/1000\n",
            "1562/1562 [==============================] - 61s 39ms/step - loss: 5.2993 - accuracy: 0.0941 - auc: 0.7319 - val_loss: 4.0928 - val_accuracy: 0.1164 - val_auc: 0.7806\n",
            "Epoch 3/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 4.5233 - accuracy: 0.1300 - auc: 0.7802 - val_loss: 3.5797 - val_accuracy: 0.1661 - val_auc: 0.8320\n",
            "Epoch 4/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 3.7581 - accuracy: 0.1623 - auc: 0.8184 - val_loss: 3.5271 - val_accuracy: 0.2094 - val_auc: 0.8677\n",
            "Epoch 5/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 3.2706 - accuracy: 0.2144 - auc: 0.8661 - val_loss: 2.9372 - val_accuracy: 0.2672 - val_auc: 0.8985\n",
            "Epoch 6/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 2.9597 - accuracy: 0.2633 - auc: 0.8940 - val_loss: 2.8229 - val_accuracy: 0.2887 - val_auc: 0.9091\n",
            "Epoch 7/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 2.7591 - accuracy: 0.2994 - auc: 0.9098 - val_loss: 2.9051 - val_accuracy: 0.3134 - val_auc: 0.9083\n",
            "Epoch 8/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 2.6041 - accuracy: 0.3307 - auc: 0.9209 - val_loss: 3.0849 - val_accuracy: 0.3019 - val_auc: 0.9064\n",
            "Epoch 9/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 2.4678 - accuracy: 0.3576 - auc: 0.9290 - val_loss: 2.4810 - val_accuracy: 0.3648 - val_auc: 0.9315\n",
            "Epoch 10/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 2.3750 - accuracy: 0.3785 - auc: 0.9341 - val_loss: 2.7580 - val_accuracy: 0.3300 - val_auc: 0.9161\n",
            "Epoch 11/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 2.2771 - accuracy: 0.3938 - auc: 0.9397 - val_loss: 2.8440 - val_accuracy: 0.3503 - val_auc: 0.9243\n",
            "Epoch 12/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 2.1742 - accuracy: 0.4207 - auc: 0.9439 - val_loss: 3.5538 - val_accuracy: 0.3820 - val_auc: 0.9297\n",
            "Epoch 13/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 2.1112 - accuracy: 0.4353 - auc: 0.9467 - val_loss: 2.3506 - val_accuracy: 0.4375 - val_auc: 0.9469\n",
            "Epoch 14/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 2.0631 - accuracy: 0.4461 - auc: 0.9494 - val_loss: 2.3103 - val_accuracy: 0.4094 - val_auc: 0.9381\n",
            "Epoch 15/1000\n",
            "1562/1562 [==============================] - 55s 35ms/step - loss: 1.9843 - accuracy: 0.4628 - auc: 0.9527 - val_loss: 2.9078 - val_accuracy: 0.4299 - val_auc: 0.9420\n",
            "Epoch 16/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 1.9259 - accuracy: 0.4741 - auc: 0.9560 - val_loss: 2.4117 - val_accuracy: 0.3959 - val_auc: 0.9343\n",
            "Epoch 17/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 1.8708 - accuracy: 0.4883 - auc: 0.9573 - val_loss: 2.2481 - val_accuracy: 0.4528 - val_auc: 0.9450\n",
            "Epoch 18/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 1.8197 - accuracy: 0.5025 - auc: 0.9598 - val_loss: 2.9895 - val_accuracy: 0.3805 - val_auc: 0.9274\n",
            "Epoch 19/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 1.7729 - accuracy: 0.5070 - auc: 0.9622 - val_loss: 2.6758 - val_accuracy: 0.4723 - val_auc: 0.9461\n",
            "Epoch 20/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 1.7088 - accuracy: 0.5245 - auc: 0.9641 - val_loss: 2.5045 - val_accuracy: 0.4057 - val_auc: 0.9275\n",
            "Epoch 21/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 1.6696 - accuracy: 0.5347 - auc: 0.9652 - val_loss: 2.1405 - val_accuracy: 0.4596 - val_auc: 0.9448\n",
            "Epoch 22/1000\n",
            "1562/1562 [==============================] - 55s 35ms/step - loss: 1.6472 - accuracy: 0.5415 - auc: 0.9659 - val_loss: 2.4013 - val_accuracy: 0.4370 - val_auc: 0.9342\n",
            "Epoch 23/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 1.6096 - accuracy: 0.5479 - auc: 0.9666 - val_loss: 2.2067 - val_accuracy: 0.4785 - val_auc: 0.9452\n",
            "Epoch 24/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 1.5615 - accuracy: 0.5595 - auc: 0.9689 - val_loss: 2.6100 - val_accuracy: 0.4742 - val_auc: 0.9438\n",
            "Epoch 25/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 1.5276 - accuracy: 0.5749 - auc: 0.9692 - val_loss: 2.0701 - val_accuracy: 0.4736 - val_auc: 0.9462\n",
            "Epoch 26/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 1.4877 - accuracy: 0.5796 - auc: 0.9710 - val_loss: 2.0838 - val_accuracy: 0.4774 - val_auc: 0.9444\n",
            "Epoch 27/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 1.4507 - accuracy: 0.5875 - auc: 0.9723 - val_loss: 1.9856 - val_accuracy: 0.4913 - val_auc: 0.9439\n",
            "Epoch 28/1000\n",
            "1562/1562 [==============================] - 61s 39ms/step - loss: 1.4222 - accuracy: 0.5943 - auc: 0.9729 - val_loss: 2.5643 - val_accuracy: 0.3774 - val_auc: 0.9117\n",
            "Epoch 29/1000\n",
            "1562/1562 [==============================] - 61s 39ms/step - loss: 1.3781 - accuracy: 0.6011 - auc: 0.9756 - val_loss: 2.4938 - val_accuracy: 0.4111 - val_auc: 0.9263\n",
            "Epoch 30/1000\n",
            "1562/1562 [==============================] - 61s 39ms/step - loss: 1.3788 - accuracy: 0.6048 - auc: 0.9736 - val_loss: 2.3071 - val_accuracy: 0.4199 - val_auc: 0.9240\n",
            "Epoch 31/1000\n",
            "1562/1562 [==============================] - 61s 39ms/step - loss: 1.3524 - accuracy: 0.6129 - auc: 0.9745 - val_loss: 1.8456 - val_accuracy: 0.5126 - val_auc: 0.9502\n",
            "Epoch 32/1000\n",
            "1562/1562 [==============================] - 62s 39ms/step - loss: 1.2902 - accuracy: 0.6245 - auc: 0.9774 - val_loss: 3.0489 - val_accuracy: 0.2857 - val_auc: 0.8756\n",
            "Epoch 33/1000\n",
            "1562/1562 [==============================] - 62s 40ms/step - loss: 1.2910 - accuracy: 0.6259 - auc: 0.9767 - val_loss: 2.0078 - val_accuracy: 0.4986 - val_auc: 0.9433\n",
            "Epoch 34/1000\n",
            "1562/1562 [==============================] - 62s 40ms/step - loss: 1.2218 - accuracy: 0.6408 - auc: 0.9802 - val_loss: 1.9354 - val_accuracy: 0.5039 - val_auc: 0.9495\n",
            "Epoch 35/1000\n",
            "1562/1562 [==============================] - 62s 40ms/step - loss: 1.2245 - accuracy: 0.6426 - auc: 0.9787 - val_loss: 2.0365 - val_accuracy: 0.4801 - val_auc: 0.9384\n",
            "Epoch 36/1000\n",
            "1562/1562 [==============================] - 61s 39ms/step - loss: 1.2051 - accuracy: 0.6449 - auc: 0.9795 - val_loss: 1.8122 - val_accuracy: 0.5322 - val_auc: 0.9502\n",
            "Epoch 37/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 1.1834 - accuracy: 0.6506 - auc: 0.9796 - val_loss: 1.8091 - val_accuracy: 0.5381 - val_auc: 0.9528\n",
            "Epoch 38/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 1.1600 - accuracy: 0.6583 - auc: 0.9806 - val_loss: 1.8909 - val_accuracy: 0.5056 - val_auc: 0.9453\n",
            "Epoch 39/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 1.1323 - accuracy: 0.6616 - auc: 0.9817 - val_loss: 2.2720 - val_accuracy: 0.5075 - val_auc: 0.9407\n",
            "Epoch 40/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 1.1019 - accuracy: 0.6709 - auc: 0.9827 - val_loss: 2.0635 - val_accuracy: 0.4764 - val_auc: 0.9347\n",
            "Epoch 41/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 1.0891 - accuracy: 0.6796 - auc: 0.9820 - val_loss: 1.9450 - val_accuracy: 0.5096 - val_auc: 0.9409\n",
            "Epoch 42/1000\n",
            "1562/1562 [==============================] - 57s 37ms/step - loss: 1.0661 - accuracy: 0.6829 - auc: 0.9828 - val_loss: 2.0901 - val_accuracy: 0.4985 - val_auc: 0.9392\n",
            "Epoch 43/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 1.0606 - accuracy: 0.6862 - auc: 0.9832 - val_loss: 1.9208 - val_accuracy: 0.5136 - val_auc: 0.9398\n",
            "Epoch 44/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 1.0362 - accuracy: 0.6923 - auc: 0.9835 - val_loss: 2.3707 - val_accuracy: 0.4531 - val_auc: 0.9161\n",
            "Epoch 45/1000\n",
            "1562/1562 [==============================] - 57s 37ms/step - loss: 1.0123 - accuracy: 0.6964 - auc: 0.9842 - val_loss: 2.0739 - val_accuracy: 0.4900 - val_auc: 0.9313\n",
            "Epoch 46/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 0.9897 - accuracy: 0.7017 - auc: 0.9848 - val_loss: 2.1594 - val_accuracy: 0.4779 - val_auc: 0.9264\n",
            "Epoch 47/1000\n",
            "1562/1562 [==============================] - 57s 37ms/step - loss: 0.9747 - accuracy: 0.7073 - auc: 0.9845 - val_loss: 1.8228 - val_accuracy: 0.5279 - val_auc: 0.9461\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 48/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 0.9113 - accuracy: 0.7226 - auc: 0.9872 - val_loss: 1.7620 - val_accuracy: 0.5560 - val_auc: 0.9456\n",
            "Epoch 49/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 0.8584 - accuracy: 0.7405 - auc: 0.9880 - val_loss: 1.9183 - val_accuracy: 0.5213 - val_auc: 0.9373\n",
            "Epoch 50/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 0.8123 - accuracy: 0.7536 - auc: 0.9894 - val_loss: 1.8926 - val_accuracy: 0.5304 - val_auc: 0.9383\n",
            "Epoch 51/1000\n",
            "1562/1562 [==============================] - 57s 37ms/step - loss: 0.7864 - accuracy: 0.7591 - auc: 0.9895 - val_loss: 1.8237 - val_accuracy: 0.5464 - val_auc: 0.9411\n",
            "Epoch 52/1000\n",
            "1562/1562 [==============================] - 57s 37ms/step - loss: 0.7818 - accuracy: 0.7586 - auc: 0.9901 - val_loss: 1.7125 - val_accuracy: 0.5639 - val_auc: 0.9481\n",
            "Epoch 53/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 0.7588 - accuracy: 0.7617 - auc: 0.9905 - val_loss: 1.7906 - val_accuracy: 0.5531 - val_auc: 0.9438\n",
            "Epoch 54/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.7376 - accuracy: 0.7710 - auc: 0.9909 - val_loss: 1.6856 - val_accuracy: 0.5718 - val_auc: 0.9484\n",
            "Epoch 55/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 0.7172 - accuracy: 0.7756 - auc: 0.9913 - val_loss: 1.7660 - val_accuracy: 0.5677 - val_auc: 0.9458\n",
            "Epoch 56/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.7130 - accuracy: 0.7763 - auc: 0.9918 - val_loss: 1.7831 - val_accuracy: 0.5574 - val_auc: 0.9428\n",
            "Epoch 57/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.7075 - accuracy: 0.7782 - auc: 0.9914 - val_loss: 1.7328 - val_accuracy: 0.5686 - val_auc: 0.9450\n",
            "Epoch 58/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.6882 - accuracy: 0.7867 - auc: 0.9910 - val_loss: 1.7849 - val_accuracy: 0.5593 - val_auc: 0.9431\n",
            "Epoch 59/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 0.6717 - accuracy: 0.7902 - auc: 0.9922 - val_loss: 1.7997 - val_accuracy: 0.5585 - val_auc: 0.9417\n",
            "Epoch 60/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 0.6650 - accuracy: 0.7956 - auc: 0.9914 - val_loss: 1.8188 - val_accuracy: 0.5639 - val_auc: 0.9392\n",
            "Epoch 61/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 0.6518 - accuracy: 0.7968 - auc: 0.9920 - val_loss: 1.7696 - val_accuracy: 0.5680 - val_auc: 0.9421\n",
            "Epoch 62/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.6477 - accuracy: 0.7982 - auc: 0.9921 - val_loss: 1.7619 - val_accuracy: 0.5673 - val_auc: 0.9430\n",
            "Epoch 63/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 0.6272 - accuracy: 0.8040 - auc: 0.9921 - val_loss: 1.8268 - val_accuracy: 0.5553 - val_auc: 0.9393\n",
            "Epoch 64/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 0.6221 - accuracy: 0.8068 - auc: 0.9924 - val_loss: 1.8011 - val_accuracy: 0.5658 - val_auc: 0.9391\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 65/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.5942 - accuracy: 0.8105 - auc: 0.9934 - val_loss: 1.7404 - val_accuracy: 0.5800 - val_auc: 0.9420\n",
            "Epoch 66/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 0.5664 - accuracy: 0.8188 - auc: 0.9934 - val_loss: 1.8240 - val_accuracy: 0.5634 - val_auc: 0.9374\n",
            "Epoch 67/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.5417 - accuracy: 0.8285 - auc: 0.9944 - val_loss: 1.7629 - val_accuracy: 0.5762 - val_auc: 0.9413\n",
            "Epoch 68/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.5386 - accuracy: 0.8269 - auc: 0.9945 - val_loss: 1.7618 - val_accuracy: 0.5758 - val_auc: 0.9415\n",
            "Epoch 69/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.5358 - accuracy: 0.8297 - auc: 0.9943 - val_loss: 1.7942 - val_accuracy: 0.5756 - val_auc: 0.9378\n",
            "Epoch 70/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.5244 - accuracy: 0.8307 - auc: 0.9944 - val_loss: 1.7497 - val_accuracy: 0.5815 - val_auc: 0.9405\n",
            "Epoch 71/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4993 - accuracy: 0.8392 - auc: 0.9951 - val_loss: 1.7807 - val_accuracy: 0.5734 - val_auc: 0.9391\n",
            "Epoch 72/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.5102 - accuracy: 0.8379 - auc: 0.9947 - val_loss: 1.7738 - val_accuracy: 0.5797 - val_auc: 0.9381\n",
            "Epoch 73/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.5061 - accuracy: 0.8378 - auc: 0.9951 - val_loss: 1.8819 - val_accuracy: 0.5623 - val_auc: 0.9338\n",
            "Epoch 74/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4930 - accuracy: 0.8440 - auc: 0.9950 - val_loss: 1.8215 - val_accuracy: 0.5737 - val_auc: 0.9369\n",
            "\n",
            "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 75/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4817 - accuracy: 0.8462 - auc: 0.9953 - val_loss: 1.7657 - val_accuracy: 0.5816 - val_auc: 0.9398\n",
            "Epoch 76/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4824 - accuracy: 0.8452 - auc: 0.9948 - val_loss: 1.7685 - val_accuracy: 0.5837 - val_auc: 0.9389\n",
            "Epoch 77/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4704 - accuracy: 0.8512 - auc: 0.9951 - val_loss: 1.7857 - val_accuracy: 0.5822 - val_auc: 0.9385\n",
            "Epoch 78/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4725 - accuracy: 0.8478 - auc: 0.9952 - val_loss: 1.7397 - val_accuracy: 0.5871 - val_auc: 0.9406\n",
            "Epoch 79/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4442 - accuracy: 0.8579 - auc: 0.9962 - val_loss: 1.7982 - val_accuracy: 0.5812 - val_auc: 0.9372\n",
            "Epoch 80/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4522 - accuracy: 0.8554 - auc: 0.9955 - val_loss: 1.7845 - val_accuracy: 0.5866 - val_auc: 0.9387\n",
            "Epoch 81/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4460 - accuracy: 0.8586 - auc: 0.9955 - val_loss: 1.7838 - val_accuracy: 0.5827 - val_auc: 0.9386\n",
            "Epoch 82/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4422 - accuracy: 0.8577 - auc: 0.9958 - val_loss: 1.7954 - val_accuracy: 0.5847 - val_auc: 0.9366\n",
            "Epoch 83/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4359 - accuracy: 0.8594 - auc: 0.9962 - val_loss: 1.8245 - val_accuracy: 0.5832 - val_auc: 0.9357\n",
            "Epoch 84/1000\n",
            "1562/1562 [==============================] - 55s 35ms/step - loss: 0.4345 - accuracy: 0.8603 - auc: 0.9957 - val_loss: 1.7532 - val_accuracy: 0.5906 - val_auc: 0.9386\n",
            "\n",
            "Epoch 00084: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 85/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4252 - accuracy: 0.8620 - auc: 0.9958 - val_loss: 1.7788 - val_accuracy: 0.5903 - val_auc: 0.9381\n",
            "Epoch 86/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4219 - accuracy: 0.8651 - auc: 0.9960 - val_loss: 1.7907 - val_accuracy: 0.5889 - val_auc: 0.9368\n",
            "Epoch 87/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4330 - accuracy: 0.8618 - auc: 0.9954 - val_loss: 1.7737 - val_accuracy: 0.5893 - val_auc: 0.9384\n",
            "Epoch 88/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 0.4259 - accuracy: 0.8626 - auc: 0.9962 - val_loss: 1.7564 - val_accuracy: 0.5902 - val_auc: 0.9389\n",
            "Epoch 89/1000\n",
            "1562/1562 [==============================] - 55s 36ms/step - loss: 0.4182 - accuracy: 0.8658 - auc: 0.9961 - val_loss: 1.7730 - val_accuracy: 0.5928 - val_auc: 0.9372\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00089: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2RkHSoVC0M7",
        "outputId": "fac1c45d-4e6f-497c-8eb6-19df15d8406d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Importing the Keras libraries and packages\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Activation\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "classifier = None\n",
        "classifier = Sequential([\n",
        "    layers.Conv2D(NUM_RECEPTIVE_FILTERS, kernel_size=GABOR_SIZE, strides=(1,1), padding='same', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.1),\n",
        "    layers.Conv2D(128, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.1),\n",
        "    layers.Conv2D(256, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.1),\n",
        "    layers.Conv2D(512, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.1),\n",
        "    layers.Conv2D(512, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    layers.Flatten(),\n",
        "    Dropout(0.1),\n",
        "    layers.Dense(4096),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.BatchNormalization(),\n",
        "    Dropout(0.1),\n",
        "    layers.Dense(4096),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.BatchNormalization(),\n",
        "    Dropout(0.1),\n",
        "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "classifier.summary()\n",
        "\n",
        "import copy\n",
        "untrained_layers = copy.deepcopy([classifier.get_layer(name=classifier.layers[GABOR_LAYER_INDEX].name).get_weights(),\n",
        "                                  classifier.get_layer(name=classifier.layers[GABOR_LAYER_INDEX+1].name).get_weights()])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,  \n",
        "                              min_delta=1e-4, mode='min', verbose=1)\n",
        "stop_alg = EarlyStopping(monitor='val_loss', patience=35, \n",
        "                         restore_best_weights=True, verbose=1)\n",
        "callbacks = [stop_alg, reduce_lr]\n",
        "opt = Adam(learning_rate=0.001)\n",
        "classifier.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', 'AUC'])\n",
        "\n",
        "start = time.perf_counter()\n",
        "hist = classifier.fit(\n",
        "    train_generator, \n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=total_validate//BATCH_SIZE,\n",
        "    steps_per_epoch=total_train//BATCH_SIZE,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "finish = time.perf_counter()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 128, 128, 32)      21632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 64, 64, 128)       36992     \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 64, 64, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 32, 32, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 32, 32, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 16, 16, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 8, 8, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 4096)              33558528  \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 100)               409700    \n",
            "=================================================================\n",
            "Total params: 54,681,828\n",
            "Trainable params: 54,662,564\n",
            "Non-trainable params: 19,264\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "1562/1562 [==============================] - 59s 37ms/step - loss: 5.7692 - accuracy: 0.0594 - auc: 0.6586 - val_loss: 4.8852 - val_accuracy: 0.1075 - val_auc: 0.7437\n",
            "Epoch 2/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 3.7905 - accuracy: 0.1747 - auc: 0.8190 - val_loss: 3.0321 - val_accuracy: 0.2613 - val_auc: 0.8845\n",
            "Epoch 3/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 2.7682 - accuracy: 0.3097 - auc: 0.9046 - val_loss: 2.6643 - val_accuracy: 0.3262 - val_auc: 0.9122\n",
            "Epoch 4/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 2.3603 - accuracy: 0.3852 - auc: 0.9339 - val_loss: 2.3619 - val_accuracy: 0.3897 - val_auc: 0.9282\n",
            "Epoch 5/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 2.0627 - accuracy: 0.4445 - auc: 0.9489 - val_loss: 2.8914 - val_accuracy: 0.3358 - val_auc: 0.8893\n",
            "Epoch 6/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 1.8343 - accuracy: 0.5015 - auc: 0.9585 - val_loss: 2.4894 - val_accuracy: 0.3813 - val_auc: 0.9178\n",
            "Epoch 7/1000\n",
            "1562/1562 [==============================] - 59s 37ms/step - loss: 1.6294 - accuracy: 0.5443 - auc: 0.9676 - val_loss: 2.1289 - val_accuracy: 0.4661 - val_auc: 0.9366\n",
            "Epoch 8/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 1.4336 - accuracy: 0.5914 - auc: 0.9743 - val_loss: 2.1012 - val_accuracy: 0.4760 - val_auc: 0.9370\n",
            "Epoch 9/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 1.2474 - accuracy: 0.6366 - auc: 0.9796 - val_loss: 2.2881 - val_accuracy: 0.4889 - val_auc: 0.9298\n",
            "Epoch 10/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 1.0738 - accuracy: 0.6824 - auc: 0.9847 - val_loss: 2.2809 - val_accuracy: 0.4699 - val_auc: 0.9212\n",
            "Epoch 11/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 0.9072 - accuracy: 0.7256 - auc: 0.9889 - val_loss: 2.2256 - val_accuracy: 0.5105 - val_auc: 0.9213\n",
            "Epoch 12/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 0.7590 - accuracy: 0.7654 - auc: 0.9908 - val_loss: 2.6206 - val_accuracy: 0.5020 - val_auc: 0.9099\n",
            "Epoch 13/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 0.6359 - accuracy: 0.8032 - auc: 0.9933 - val_loss: 2.8305 - val_accuracy: 0.4901 - val_auc: 0.8984\n",
            "Epoch 14/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.5232 - accuracy: 0.8325 - auc: 0.9950 - val_loss: 2.6422 - val_accuracy: 0.5025 - val_auc: 0.8960\n",
            "Epoch 15/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.4268 - accuracy: 0.8624 - auc: 0.9961 - val_loss: 2.7053 - val_accuracy: 0.4958 - val_auc: 0.8911\n",
            "Epoch 16/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.3675 - accuracy: 0.8791 - auc: 0.9967 - val_loss: 2.9406 - val_accuracy: 0.5171 - val_auc: 0.8832\n",
            "Epoch 17/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 0.3574 - accuracy: 0.8838 - auc: 0.9960 - val_loss: 2.8532 - val_accuracy: 0.5081 - val_auc: 0.8841\n",
            "Epoch 18/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.3038 - accuracy: 0.9006 - auc: 0.9971 - val_loss: 3.3488 - val_accuracy: 0.5063 - val_auc: 0.8732\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 19/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.1854 - accuracy: 0.9379 - auc: 0.9989 - val_loss: 2.6295 - val_accuracy: 0.5472 - val_auc: 0.8969\n",
            "Epoch 20/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 0.1111 - accuracy: 0.9644 - auc: 0.9993 - val_loss: 3.3158 - val_accuracy: 0.5463 - val_auc: 0.8840\n",
            "Epoch 21/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.1000 - accuracy: 0.9678 - auc: 0.9992 - val_loss: 2.9911 - val_accuracy: 0.5386 - val_auc: 0.8782\n",
            "Epoch 22/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.0943 - accuracy: 0.9687 - auc: 0.9994 - val_loss: 2.9918 - val_accuracy: 0.5392 - val_auc: 0.8782\n",
            "Epoch 23/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 0.0934 - accuracy: 0.9686 - auc: 0.9994 - val_loss: 3.0932 - val_accuracy: 0.5356 - val_auc: 0.8755\n",
            "Epoch 24/1000\n",
            "1562/1562 [==============================] - 60s 39ms/step - loss: 0.0822 - accuracy: 0.9719 - auc: 0.9996 - val_loss: 3.1481 - val_accuracy: 0.5450 - val_auc: 0.8733\n",
            "Epoch 25/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 0.0759 - accuracy: 0.9754 - auc: 0.9996 - val_loss: 3.0861 - val_accuracy: 0.5495 - val_auc: 0.8759\n",
            "Epoch 26/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.0750 - accuracy: 0.9752 - auc: 0.9995 - val_loss: 3.1376 - val_accuracy: 0.5472 - val_auc: 0.8743\n",
            "Epoch 27/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 0.0784 - accuracy: 0.9747 - auc: 0.9992 - val_loss: 3.0789 - val_accuracy: 0.5514 - val_auc: 0.8780\n",
            "Epoch 28/1000\n",
            "1562/1562 [==============================] - 59s 38ms/step - loss: 0.0701 - accuracy: 0.9773 - auc: 0.9993 - val_loss: 3.3251 - val_accuracy: 0.5311 - val_auc: 0.8663\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 29/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.0545 - accuracy: 0.9815 - auc: 0.9996 - val_loss: 3.0524 - val_accuracy: 0.5672 - val_auc: 0.8786\n",
            "Epoch 30/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.0396 - accuracy: 0.9872 - auc: 0.9997 - val_loss: 3.1487 - val_accuracy: 0.5654 - val_auc: 0.8767\n",
            "Epoch 31/1000\n",
            "1562/1562 [==============================] - 60s 39ms/step - loss: 0.0302 - accuracy: 0.9907 - auc: 0.9998 - val_loss: 3.1945 - val_accuracy: 0.5637 - val_auc: 0.8759\n",
            "Epoch 32/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.0313 - accuracy: 0.9896 - auc: 0.9998 - val_loss: 3.2411 - val_accuracy: 0.5616 - val_auc: 0.8738\n",
            "Epoch 33/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.0246 - accuracy: 0.9921 - auc: 0.9998 - val_loss: 3.1675 - val_accuracy: 0.5655 - val_auc: 0.8754\n",
            "Epoch 34/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.0265 - accuracy: 0.9918 - auc: 0.9998 - val_loss: 3.2940 - val_accuracy: 0.5683 - val_auc: 0.8728\n",
            "Epoch 35/1000\n",
            "1562/1562 [==============================] - 60s 39ms/step - loss: 0.0250 - accuracy: 0.9914 - auc: 0.9999 - val_loss: 3.1884 - val_accuracy: 0.5681 - val_auc: 0.8733\n",
            "Epoch 36/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.0292 - accuracy: 0.9904 - auc: 0.9997 - val_loss: 3.2713 - val_accuracy: 0.5679 - val_auc: 0.8726\n",
            "Epoch 37/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.0255 - accuracy: 0.9912 - auc: 0.9998 - val_loss: 3.2708 - val_accuracy: 0.5688 - val_auc: 0.8709\n",
            "Epoch 38/1000\n",
            "1562/1562 [==============================] - 60s 38ms/step - loss: 0.0218 - accuracy: 0.9934 - auc: 0.9998 - val_loss: 3.3315 - val_accuracy: 0.5589 - val_auc: 0.8677\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 39/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 0.0227 - accuracy: 0.9932 - auc: 0.9998 - val_loss: 3.2781 - val_accuracy: 0.5733 - val_auc: 0.8715\n",
            "Epoch 40/1000\n",
            "1562/1562 [==============================] - 58s 37ms/step - loss: 0.0147 - accuracy: 0.9954 - auc: 0.9999 - val_loss: 3.2563 - val_accuracy: 0.5719 - val_auc: 0.8734\n",
            "Epoch 41/1000\n",
            "1562/1562 [==============================] - 57s 37ms/step - loss: 0.0141 - accuracy: 0.9958 - auc: 0.9999 - val_loss: 3.2936 - val_accuracy: 0.5748 - val_auc: 0.8726\n",
            "Epoch 42/1000\n",
            "1562/1562 [==============================] - 57s 36ms/step - loss: 0.0128 - accuracy: 0.9963 - auc: 0.9999 - val_loss: 3.2467 - val_accuracy: 0.5769 - val_auc: 0.8739\n",
            "Epoch 43/1000\n",
            "1562/1562 [==============================] - 56s 36ms/step - loss: 0.0137 - accuracy: 0.9963 - auc: 0.9999 - val_loss: 3.3023 - val_accuracy: 0.5780 - val_auc: 0.8721\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00043: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfZwqEPprFmZ",
        "outputId": "c233a259-fe5b-4b7d-acfa-8458bd25ae8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Importing the Keras libraries and packages\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Activation\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "classifier = None\n",
        "classifier = Sequential([\n",
        "    layers.Conv2D(NUM_RECEPTIVE_FILTERS, kernel_size=GABOR_SIZE, strides=(1,1), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.25),\n",
        "    layers.Conv2D(64, kernel_size=(3,3), strides=(2,2), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.25),\n",
        "    layers.Conv2D(128, kernel_size=(3,3), strides=(2,2), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    Dropout(0.25),\n",
        "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "classifier.summary()\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,  \n",
        "                              min_delta=1e-4, mode='min', verbose=1)\n",
        "stop_alg = EarlyStopping(monitor='val_loss', patience=35, \n",
        "                         restore_best_weights=True, verbose=1)\n",
        "callbacks = [stop_alg, reduce_lr]\n",
        "opt = Adam(learning_rate=0.001)\n",
        "classifier.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', 'AUC'])\n",
        "\n",
        "start = time.perf_counter()\n",
        "hist = classifier.fit(\n",
        "    train_generator, \n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=total_validate//BATCH_SIZE,\n",
        "    steps_per_epoch=total_train//BATCH_SIZE,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "finish = time.perf_counter()\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 114, 114, 32)      21632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 114, 114, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 57, 57, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 57, 57, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 28, 28, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 6, 6, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 512)               590336    \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               51300     \n",
            "=================================================================\n",
            "Total params: 758,564\n",
            "Trainable params: 757,092\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "1562/1562 [==============================] - 50s 32ms/step - loss: 4.3113 - accuracy: 0.0863 - auc: 0.7240 - val_loss: 3.3859 - val_accuracy: 0.1989 - val_auc: 0.8540\n",
            "Epoch 2/1000\n",
            "1562/1562 [==============================] - 50s 32ms/step - loss: 3.2813 - accuracy: 0.2125 - auc: 0.8643 - val_loss: 3.1328 - val_accuracy: 0.2445 - val_auc: 0.8742\n",
            "Epoch 3/1000\n",
            "1562/1562 [==============================] - 51s 33ms/step - loss: 2.9124 - accuracy: 0.2802 - auc: 0.8977 - val_loss: 2.7863 - val_accuracy: 0.3092 - val_auc: 0.9035\n",
            "Epoch 4/1000\n",
            "1562/1562 [==============================] - 51s 32ms/step - loss: 2.6528 - accuracy: 0.3296 - auc: 0.9151 - val_loss: 2.6378 - val_accuracy: 0.3365 - val_auc: 0.9119\n",
            "Epoch 5/1000\n",
            "1562/1562 [==============================] - 49s 31ms/step - loss: 2.4748 - accuracy: 0.3646 - auc: 0.9274 - val_loss: 2.3596 - val_accuracy: 0.3981 - val_auc: 0.9311\n",
            "Epoch 6/1000\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 2.3214 - accuracy: 0.4017 - auc: 0.9356 - val_loss: 2.3288 - val_accuracy: 0.4057 - val_auc: 0.9306\n",
            "Epoch 7/1000\n",
            "1562/1562 [==============================] - 47s 30ms/step - loss: 2.1995 - accuracy: 0.4212 - auc: 0.9434 - val_loss: 2.2529 - val_accuracy: 0.4205 - val_auc: 0.9337\n",
            "Epoch 8/1000\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 2.0878 - accuracy: 0.4477 - auc: 0.9474 - val_loss: 2.2329 - val_accuracy: 0.4308 - val_auc: 0.9358\n",
            "Epoch 9/1000\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 1.9930 - accuracy: 0.4705 - auc: 0.9521 - val_loss: 2.3071 - val_accuracy: 0.4173 - val_auc: 0.9277\n",
            "Epoch 10/1000\n",
            "1562/1562 [==============================] - 49s 31ms/step - loss: 1.9327 - accuracy: 0.4779 - auc: 0.9563 - val_loss: 2.1864 - val_accuracy: 0.4374 - val_auc: 0.9344\n",
            "Epoch 11/1000\n",
            "1562/1562 [==============================] - 49s 31ms/step - loss: 1.8592 - accuracy: 0.4973 - auc: 0.9587 - val_loss: 2.2003 - val_accuracy: 0.4373 - val_auc: 0.9337\n",
            "Epoch 12/1000\n",
            "1562/1562 [==============================] - 48s 30ms/step - loss: 1.7988 - accuracy: 0.5128 - auc: 0.9610 - val_loss: 2.4357 - val_accuracy: 0.3983 - val_auc: 0.9159\n",
            "Epoch 13/1000\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 1.7494 - accuracy: 0.5222 - auc: 0.9622 - val_loss: 2.1688 - val_accuracy: 0.4473 - val_auc: 0.9329\n",
            "Epoch 14/1000\n",
            "1562/1562 [==============================] - 47s 30ms/step - loss: 1.6894 - accuracy: 0.5336 - auc: 0.9654 - val_loss: 2.1135 - val_accuracy: 0.4610 - val_auc: 0.9369\n",
            "Epoch 15/1000\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 1.6601 - accuracy: 0.5435 - auc: 0.9656 - val_loss: 2.1580 - val_accuracy: 0.4562 - val_auc: 0.9325\n",
            "Epoch 16/1000\n",
            "1562/1562 [==============================] - 47s 30ms/step - loss: 1.6039 - accuracy: 0.5564 - auc: 0.9684 - val_loss: 2.1626 - val_accuracy: 0.4565 - val_auc: 0.9303\n",
            "Epoch 17/1000\n",
            "1562/1562 [==============================] - 49s 31ms/step - loss: 1.5637 - accuracy: 0.5637 - auc: 0.9692 - val_loss: 2.1467 - val_accuracy: 0.4575 - val_auc: 0.9329\n",
            "Epoch 18/1000\n",
            "1562/1562 [==============================] - 47s 30ms/step - loss: 1.5431 - accuracy: 0.5658 - auc: 0.9702 - val_loss: 2.0964 - val_accuracy: 0.4728 - val_auc: 0.9338\n",
            "Epoch 19/1000\n",
            "1562/1562 [==============================] - 47s 30ms/step - loss: 1.4997 - accuracy: 0.5812 - auc: 0.9714 - val_loss: 2.0764 - val_accuracy: 0.4779 - val_auc: 0.9341\n",
            "Epoch 20/1000\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 1.4609 - accuracy: 0.5874 - auc: 0.9733 - val_loss: 2.0425 - val_accuracy: 0.4802 - val_auc: 0.9369\n",
            "Epoch 21/1000\n",
            "1562/1562 [==============================] - 47s 30ms/step - loss: 1.4309 - accuracy: 0.5936 - auc: 0.9742 - val_loss: 2.1419 - val_accuracy: 0.4698 - val_auc: 0.9295\n",
            "Epoch 22/1000\n",
            "1562/1562 [==============================] - 49s 31ms/step - loss: 1.4272 - accuracy: 0.5930 - auc: 0.9748 - val_loss: 2.2344 - val_accuracy: 0.4576 - val_auc: 0.9231\n",
            "Epoch 23/1000\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 1.4066 - accuracy: 0.5984 - auc: 0.9754 - val_loss: 2.1988 - val_accuracy: 0.4554 - val_auc: 0.9275\n",
            "Epoch 24/1000\n",
            "1562/1562 [==============================] - 48s 30ms/step - loss: 1.3796 - accuracy: 0.6098 - auc: 0.9761 - val_loss: 2.1404 - val_accuracy: 0.4763 - val_auc: 0.9272\n",
            "Epoch 25/1000\n",
            "1562/1562 [==============================] - 49s 31ms/step - loss: 1.3486 - accuracy: 0.6136 - auc: 0.9773 - val_loss: 2.3634 - val_accuracy: 0.4324 - val_auc: 0.9157\n",
            "Epoch 26/1000\n",
            "1562/1562 [==============================] - 47s 30ms/step - loss: 1.3286 - accuracy: 0.6185 - auc: 0.9776 - val_loss: 2.0880 - val_accuracy: 0.4859 - val_auc: 0.9317\n",
            "Epoch 27/1000\n",
            "1562/1562 [==============================] - 48s 30ms/step - loss: 1.3197 - accuracy: 0.6207 - auc: 0.9772 - val_loss: 2.2398 - val_accuracy: 0.4533 - val_auc: 0.9231\n",
            "Epoch 28/1000\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 1.2917 - accuracy: 0.6244 - auc: 0.9785 - val_loss: 2.1519 - val_accuracy: 0.4724 - val_auc: 0.9270\n",
            "Epoch 29/1000\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 1.2919 - accuracy: 0.6314 - auc: 0.9783 - val_loss: 2.1547 - val_accuracy: 0.4683 - val_auc: 0.9270\n",
            "Epoch 30/1000\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 1.2637 - accuracy: 0.6335 - auc: 0.9788 - val_loss: 2.1028 - val_accuracy: 0.4790 - val_auc: 0.9300\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 31/1000\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 1.1836 - accuracy: 0.6551 - auc: 0.9824 - val_loss: 2.0631 - val_accuracy: 0.4912 - val_auc: 0.9317\n",
            "Epoch 32/1000\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 1.1419 - accuracy: 0.6677 - auc: 0.9829 - val_loss: 2.0641 - val_accuracy: 0.4949 - val_auc: 0.9314\n",
            "Epoch 33/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.1161 - accuracy: 0.6718 - auc: 0.9837 - val_loss: 2.0727 - val_accuracy: 0.4981 - val_auc: 0.9288\n",
            "Epoch 34/1000\n",
            "1562/1562 [==============================] - 46s 30ms/step - loss: 1.0948 - accuracy: 0.6773 - auc: 0.9845 - val_loss: 2.0762 - val_accuracy: 0.4946 - val_auc: 0.9298\n",
            "Epoch 35/1000\n",
            "1562/1562 [==============================] - 47s 30ms/step - loss: 1.0763 - accuracy: 0.6810 - auc: 0.9854 - val_loss: 2.1010 - val_accuracy: 0.4939 - val_auc: 0.9278\n",
            "Epoch 36/1000\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 1.0581 - accuracy: 0.6862 - auc: 0.9853 - val_loss: 2.1431 - val_accuracy: 0.4911 - val_auc: 0.9244\n",
            "Epoch 37/1000\n",
            "1562/1562 [==============================] - 49s 31ms/step - loss: 1.0481 - accuracy: 0.6894 - auc: 0.9850 - val_loss: 2.1050 - val_accuracy: 0.4939 - val_auc: 0.9266\n",
            "Epoch 38/1000\n",
            "1562/1562 [==============================] - 50s 32ms/step - loss: 1.0468 - accuracy: 0.6867 - auc: 0.9851 - val_loss: 2.1079 - val_accuracy: 0.4943 - val_auc: 0.9268\n",
            "Epoch 39/1000\n",
            "1562/1562 [==============================] - 50s 32ms/step - loss: 1.0285 - accuracy: 0.6916 - auc: 0.9858 - val_loss: 2.1280 - val_accuracy: 0.4958 - val_auc: 0.9252\n",
            "Epoch 40/1000\n",
            "1562/1562 [==============================] - 50s 32ms/step - loss: 1.0366 - accuracy: 0.6932 - auc: 0.9846 - val_loss: 2.1557 - val_accuracy: 0.4913 - val_auc: 0.9225\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 41/1000\n",
            "1562/1562 [==============================] - 52s 33ms/step - loss: 0.9924 - accuracy: 0.7050 - auc: 0.9864 - val_loss: 2.1114 - val_accuracy: 0.4991 - val_auc: 0.9253\n",
            "Epoch 42/1000\n",
            "1562/1562 [==============================] - 52s 33ms/step - loss: 0.9820 - accuracy: 0.7048 - auc: 0.9873 - val_loss: 2.1084 - val_accuracy: 0.5027 - val_auc: 0.9256\n",
            "Epoch 43/1000\n",
            "1562/1562 [==============================] - 51s 33ms/step - loss: 0.9700 - accuracy: 0.7065 - auc: 0.9870 - val_loss: 2.0950 - val_accuracy: 0.5015 - val_auc: 0.9265\n",
            "Epoch 44/1000\n",
            "1562/1562 [==============================] - 52s 33ms/step - loss: 0.9407 - accuracy: 0.7190 - auc: 0.9878 - val_loss: 2.1029 - val_accuracy: 0.5028 - val_auc: 0.9262\n",
            "Epoch 45/1000\n",
            "1562/1562 [==============================] - 52s 33ms/step - loss: 0.9548 - accuracy: 0.7098 - auc: 0.9878 - val_loss: 2.1024 - val_accuracy: 0.5004 - val_auc: 0.9254\n",
            "Epoch 46/1000\n",
            "1562/1562 [==============================] - 52s 33ms/step - loss: 0.9322 - accuracy: 0.7178 - auc: 0.9878 - val_loss: 2.1480 - val_accuracy: 0.4959 - val_auc: 0.9225\n",
            "Epoch 47/1000\n",
            "1562/1562 [==============================] - 52s 33ms/step - loss: 0.9223 - accuracy: 0.7211 - auc: 0.9881 - val_loss: 2.1329 - val_accuracy: 0.5025 - val_auc: 0.9230\n",
            "Epoch 48/1000\n",
            "1562/1562 [==============================] - 53s 34ms/step - loss: 0.9237 - accuracy: 0.7240 - auc: 0.9874 - val_loss: 2.1149 - val_accuracy: 0.5022 - val_auc: 0.9234\n",
            "Epoch 49/1000\n",
            "1562/1562 [==============================] - 52s 33ms/step - loss: 0.9332 - accuracy: 0.7199 - auc: 0.9875 - val_loss: 2.1422 - val_accuracy: 0.4962 - val_auc: 0.9225\n",
            "Epoch 50/1000\n",
            "1562/1562 [==============================] - 52s 33ms/step - loss: 0.9319 - accuracy: 0.7188 - auc: 0.9876 - val_loss: 2.1276 - val_accuracy: 0.5016 - val_auc: 0.9235\n",
            "\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 51/1000\n",
            "1562/1562 [==============================] - 52s 34ms/step - loss: 0.8893 - accuracy: 0.7313 - auc: 0.9888 - val_loss: 2.1076 - val_accuracy: 0.5052 - val_auc: 0.9242\n",
            "Epoch 52/1000\n",
            "1562/1562 [==============================] - 52s 34ms/step - loss: 0.8910 - accuracy: 0.7297 - auc: 0.9884 - val_loss: 2.1070 - val_accuracy: 0.5019 - val_auc: 0.9246\n",
            "Epoch 53/1000\n",
            "1562/1562 [==============================] - 52s 34ms/step - loss: 0.8900 - accuracy: 0.7308 - auc: 0.9884 - val_loss: 2.1081 - val_accuracy: 0.5046 - val_auc: 0.9243\n",
            "Epoch 54/1000\n",
            "1562/1562 [==============================] - 53s 34ms/step - loss: 0.8703 - accuracy: 0.7353 - auc: 0.9892 - val_loss: 2.1219 - val_accuracy: 0.5040 - val_auc: 0.9238\n",
            "Epoch 55/1000\n",
            "1562/1562 [==============================] - 52s 33ms/step - loss: 0.8841 - accuracy: 0.7332 - auc: 0.9886 - val_loss: 2.1230 - val_accuracy: 0.5026 - val_auc: 0.9239\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00055: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GQfZk_J9L4A-",
        "outputId": "6a31c6b9-2735-46b5-8bda-e2114df73892"
      },
      "source": [
        "# Importing the Keras libraries and packages\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Activation\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "classifier = None\n",
        "classifier = Sequential([\n",
        "    layers.Conv2D(NUM_RECEPTIVE_FILTERS, kernel_size=GABOR_SIZE, strides=(1,1), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.25),\n",
        "    layers.Conv2D(64, kernel_size=(3,3), strides=(2,2), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.25),\n",
        "    layers.Conv2D(128, kernel_size=(3,3), strides=(2,2), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    Dropout(0.25),\n",
        "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "classifier.summary()\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,  \n",
        "                              min_delta=1e-4, mode='min', verbose=1)\n",
        "stop_alg = EarlyStopping(monitor='val_loss', patience=35, \n",
        "                         restore_best_weights=True, verbose=1)\n",
        "callbacks = [stop_alg, reduce_lr]\n",
        "opt = Adam(learning_rate=0.001)\n",
        "classifier.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', 'AUC'])\n",
        "\n",
        "start = time.perf_counter()\n",
        "hist = classifier.fit(\n",
        "    train_generator, \n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=total_validate//BATCH_SIZE,\n",
        "    steps_per_epoch=total_train//BATCH_SIZE,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "finish = time.perf_counter()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 114, 114, 32)      21632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 114, 114, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 57, 57, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 57, 57, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 28, 28, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 6, 6, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 512)               590336    \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               51300     \n",
            "=================================================================\n",
            "Total params: 758,564\n",
            "Trainable params: 757,092\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 4.8480 - accuracy: 0.0491 - auc: 0.6597 - val_loss: 3.8872 - val_accuracy: 0.1053 - val_auc: 0.7918\n",
            "Epoch 2/1000\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 3.7714 - accuracy: 0.1323 - auc: 0.8108 - val_loss: 3.5530 - val_accuracy: 0.1627 - val_auc: 0.8402\n",
            "Epoch 3/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 3.4899 - accuracy: 0.1714 - auc: 0.8448 - val_loss: 3.3825 - val_accuracy: 0.1910 - val_auc: 0.8568\n",
            "Epoch 4/1000\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 3.3075 - accuracy: 0.2016 - auc: 0.8650 - val_loss: 3.1536 - val_accuracy: 0.2357 - val_auc: 0.8781\n",
            "Epoch 5/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 3.1806 - accuracy: 0.2272 - auc: 0.8768 - val_loss: 3.0841 - val_accuracy: 0.2474 - val_auc: 0.8850\n",
            "Epoch 6/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 3.0988 - accuracy: 0.2430 - auc: 0.8833 - val_loss: 3.0529 - val_accuracy: 0.2540 - val_auc: 0.8874\n",
            "Epoch 7/1000\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 3.0128 - accuracy: 0.2563 - auc: 0.8904 - val_loss: 2.8519 - val_accuracy: 0.2946 - val_auc: 0.9034\n",
            "Epoch 8/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.9416 - accuracy: 0.2721 - auc: 0.8976 - val_loss: 2.7949 - val_accuracy: 0.3112 - val_auc: 0.9086\n",
            "Epoch 9/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.8686 - accuracy: 0.2892 - auc: 0.9022 - val_loss: 2.6959 - val_accuracy: 0.3268 - val_auc: 0.9161\n",
            "Epoch 10/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.8486 - accuracy: 0.2912 - auc: 0.9039 - val_loss: 2.6682 - val_accuracy: 0.3269 - val_auc: 0.9192\n",
            "Epoch 11/1000\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 2.7751 - accuracy: 0.3050 - auc: 0.9086 - val_loss: 2.6353 - val_accuracy: 0.3351 - val_auc: 0.9175\n",
            "Epoch 12/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.7543 - accuracy: 0.3107 - auc: 0.9097 - val_loss: 2.5716 - val_accuracy: 0.3451 - val_auc: 0.9221\n",
            "Epoch 13/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.7180 - accuracy: 0.3164 - auc: 0.9122 - val_loss: 2.7248 - val_accuracy: 0.3237 - val_auc: 0.9090\n",
            "Epoch 14/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.6705 - accuracy: 0.3261 - auc: 0.9165 - val_loss: 2.5147 - val_accuracy: 0.3722 - val_auc: 0.9234\n",
            "Epoch 15/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.6563 - accuracy: 0.3293 - auc: 0.9171 - val_loss: 2.6869 - val_accuracy: 0.3328 - val_auc: 0.9095\n",
            "Epoch 16/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.6335 - accuracy: 0.3322 - auc: 0.9178 - val_loss: 2.4482 - val_accuracy: 0.3784 - val_auc: 0.9278\n",
            "Epoch 17/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.6003 - accuracy: 0.3391 - auc: 0.9201 - val_loss: 2.4767 - val_accuracy: 0.3774 - val_auc: 0.9277\n",
            "Epoch 18/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.5751 - accuracy: 0.3444 - auc: 0.9218 - val_loss: 2.3425 - val_accuracy: 0.4000 - val_auc: 0.9357\n",
            "Epoch 19/1000\n",
            "1562/1562 [==============================] - 45s 28ms/step - loss: 2.5550 - accuracy: 0.3553 - auc: 0.9225 - val_loss: 2.4006 - val_accuracy: 0.3908 - val_auc: 0.9315\n",
            "Epoch 20/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.5314 - accuracy: 0.3548 - auc: 0.9247 - val_loss: 2.4151 - val_accuracy: 0.3842 - val_auc: 0.9297\n",
            "Epoch 21/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.5084 - accuracy: 0.3609 - auc: 0.9257 - val_loss: 2.2973 - val_accuracy: 0.4067 - val_auc: 0.9382\n",
            "Epoch 22/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.5027 - accuracy: 0.3610 - auc: 0.9259 - val_loss: 2.3336 - val_accuracy: 0.4045 - val_auc: 0.9352\n",
            "Epoch 23/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.4727 - accuracy: 0.3710 - auc: 0.9277 - val_loss: 2.3764 - val_accuracy: 0.3985 - val_auc: 0.9315\n",
            "Epoch 24/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.4599 - accuracy: 0.3686 - auc: 0.9285 - val_loss: 2.4796 - val_accuracy: 0.3714 - val_auc: 0.9238\n",
            "Epoch 25/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.4579 - accuracy: 0.3700 - auc: 0.9294 - val_loss: 2.2906 - val_accuracy: 0.4127 - val_auc: 0.9366\n",
            "Epoch 26/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.4276 - accuracy: 0.3757 - auc: 0.9307 - val_loss: 2.2373 - val_accuracy: 0.4270 - val_auc: 0.9409\n",
            "Epoch 27/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.4121 - accuracy: 0.3838 - auc: 0.9313 - val_loss: 2.3004 - val_accuracy: 0.4193 - val_auc: 0.9353\n",
            "Epoch 28/1000\n",
            "1562/1562 [==============================] - 45s 28ms/step - loss: 2.4124 - accuracy: 0.3866 - auc: 0.9306 - val_loss: 2.3763 - val_accuracy: 0.4003 - val_auc: 0.9305\n",
            "Epoch 29/1000\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 2.3941 - accuracy: 0.3826 - auc: 0.9322 - val_loss: 2.2918 - val_accuracy: 0.4102 - val_auc: 0.9367\n",
            "Epoch 30/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.3898 - accuracy: 0.3864 - auc: 0.9322 - val_loss: 2.2341 - val_accuracy: 0.4253 - val_auc: 0.9410\n",
            "Epoch 31/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.3698 - accuracy: 0.3874 - auc: 0.9337 - val_loss: 2.2362 - val_accuracy: 0.4279 - val_auc: 0.9402\n",
            "Epoch 32/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.3539 - accuracy: 0.3879 - auc: 0.9348 - val_loss: 2.2208 - val_accuracy: 0.4314 - val_auc: 0.9399\n",
            "Epoch 33/1000\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 2.3471 - accuracy: 0.3918 - auc: 0.9348 - val_loss: 2.2148 - val_accuracy: 0.4365 - val_auc: 0.9401\n",
            "Epoch 34/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.3452 - accuracy: 0.3928 - auc: 0.9353 - val_loss: 2.1217 - val_accuracy: 0.4471 - val_auc: 0.9469\n",
            "Epoch 35/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.3316 - accuracy: 0.4001 - auc: 0.9354 - val_loss: 2.1687 - val_accuracy: 0.4389 - val_auc: 0.9433\n",
            "Epoch 36/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.3348 - accuracy: 0.3968 - auc: 0.9355 - val_loss: 2.2625 - val_accuracy: 0.4123 - val_auc: 0.9378\n",
            "Epoch 37/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.3186 - accuracy: 0.4007 - auc: 0.9363 - val_loss: 2.1587 - val_accuracy: 0.4396 - val_auc: 0.9437\n",
            "Epoch 38/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.3221 - accuracy: 0.4009 - auc: 0.9359 - val_loss: 2.1496 - val_accuracy: 0.4456 - val_auc: 0.9435\n",
            "Epoch 39/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.3272 - accuracy: 0.3966 - auc: 0.9355 - val_loss: 2.1775 - val_accuracy: 0.4321 - val_auc: 0.9422\n",
            "Epoch 40/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.3025 - accuracy: 0.4032 - auc: 0.9366 - val_loss: 2.1246 - val_accuracy: 0.4455 - val_auc: 0.9457\n",
            "Epoch 41/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2868 - accuracy: 0.4068 - auc: 0.9378 - val_loss: 2.2509 - val_accuracy: 0.4150 - val_auc: 0.9380\n",
            "Epoch 42/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2797 - accuracy: 0.4091 - auc: 0.9377 - val_loss: 2.1519 - val_accuracy: 0.4446 - val_auc: 0.9429\n",
            "Epoch 43/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2834 - accuracy: 0.4028 - auc: 0.9385 - val_loss: 2.1299 - val_accuracy: 0.4462 - val_auc: 0.9446\n",
            "Epoch 44/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2735 - accuracy: 0.4096 - auc: 0.9383 - val_loss: 2.0800 - val_accuracy: 0.4565 - val_auc: 0.9485\n",
            "Epoch 45/1000\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 2.2599 - accuracy: 0.4108 - auc: 0.9400 - val_loss: 2.0699 - val_accuracy: 0.4560 - val_auc: 0.9503\n",
            "Epoch 46/1000\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 2.2582 - accuracy: 0.4114 - auc: 0.9399 - val_loss: 2.3310 - val_accuracy: 0.4072 - val_auc: 0.9321\n",
            "Epoch 47/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2479 - accuracy: 0.4133 - auc: 0.9400 - val_loss: 2.1140 - val_accuracy: 0.4481 - val_auc: 0.9459\n",
            "Epoch 48/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2735 - accuracy: 0.4077 - auc: 0.9386 - val_loss: 2.0731 - val_accuracy: 0.4560 - val_auc: 0.9477\n",
            "Epoch 49/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2648 - accuracy: 0.4078 - auc: 0.9393 - val_loss: 2.0895 - val_accuracy: 0.4539 - val_auc: 0.9476\n",
            "Epoch 50/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2414 - accuracy: 0.4116 - auc: 0.9396 - val_loss: 2.1426 - val_accuracy: 0.4462 - val_auc: 0.9428\n",
            "Epoch 51/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2361 - accuracy: 0.4186 - auc: 0.9401 - val_loss: 2.0716 - val_accuracy: 0.4610 - val_auc: 0.9474\n",
            "Epoch 52/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2225 - accuracy: 0.4215 - auc: 0.9415 - val_loss: 2.1070 - val_accuracy: 0.4499 - val_auc: 0.9450\n",
            "Epoch 53/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2246 - accuracy: 0.4160 - auc: 0.9415 - val_loss: 2.0680 - val_accuracy: 0.4595 - val_auc: 0.9483\n",
            "Epoch 54/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2228 - accuracy: 0.4181 - auc: 0.9421 - val_loss: 2.0234 - val_accuracy: 0.4673 - val_auc: 0.9495\n",
            "Epoch 55/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2028 - accuracy: 0.4219 - auc: 0.9420 - val_loss: 2.0333 - val_accuracy: 0.4615 - val_auc: 0.9495\n",
            "Epoch 56/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1961 - accuracy: 0.4202 - auc: 0.9427 - val_loss: 2.0807 - val_accuracy: 0.4589 - val_auc: 0.9468\n",
            "Epoch 57/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.2104 - accuracy: 0.4222 - auc: 0.9412 - val_loss: 2.0708 - val_accuracy: 0.4590 - val_auc: 0.9476\n",
            "Epoch 58/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1973 - accuracy: 0.4240 - auc: 0.9428 - val_loss: 2.0344 - val_accuracy: 0.4681 - val_auc: 0.9497\n",
            "Epoch 59/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1931 - accuracy: 0.4258 - auc: 0.9429 - val_loss: 2.1400 - val_accuracy: 0.4470 - val_auc: 0.9442\n",
            "Epoch 60/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1794 - accuracy: 0.4267 - auc: 0.9440 - val_loss: 2.0106 - val_accuracy: 0.4702 - val_auc: 0.9502\n",
            "Epoch 61/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1829 - accuracy: 0.4246 - auc: 0.9432 - val_loss: 2.0896 - val_accuracy: 0.4545 - val_auc: 0.9469\n",
            "Epoch 62/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1858 - accuracy: 0.4275 - auc: 0.9429 - val_loss: 2.1089 - val_accuracy: 0.4514 - val_auc: 0.9461\n",
            "Epoch 63/1000\n",
            "1562/1562 [==============================] - 45s 28ms/step - loss: 2.1687 - accuracy: 0.4297 - auc: 0.9433 - val_loss: 2.0504 - val_accuracy: 0.4654 - val_auc: 0.9477\n",
            "Epoch 64/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1915 - accuracy: 0.4254 - auc: 0.9425 - val_loss: 2.0723 - val_accuracy: 0.4568 - val_auc: 0.9475\n",
            "Epoch 65/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1650 - accuracy: 0.4306 - auc: 0.9440 - val_loss: 2.0518 - val_accuracy: 0.4655 - val_auc: 0.9483\n",
            "Epoch 66/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1675 - accuracy: 0.4312 - auc: 0.9434 - val_loss: 1.9962 - val_accuracy: 0.4729 - val_auc: 0.9516\n",
            "Epoch 67/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1570 - accuracy: 0.4330 - auc: 0.9451 - val_loss: 2.0243 - val_accuracy: 0.4731 - val_auc: 0.9498\n",
            "Epoch 68/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1715 - accuracy: 0.4300 - auc: 0.9429 - val_loss: 2.2500 - val_accuracy: 0.4264 - val_auc: 0.9352\n",
            "Epoch 69/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1562 - accuracy: 0.4349 - auc: 0.9440 - val_loss: 1.9986 - val_accuracy: 0.4767 - val_auc: 0.9503\n",
            "Epoch 70/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1620 - accuracy: 0.4300 - auc: 0.9436 - val_loss: 2.0734 - val_accuracy: 0.4596 - val_auc: 0.9476\n",
            "Epoch 71/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1547 - accuracy: 0.4331 - auc: 0.9450 - val_loss: 1.9824 - val_accuracy: 0.4765 - val_auc: 0.9509\n",
            "Epoch 72/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1337 - accuracy: 0.4386 - auc: 0.9458 - val_loss: 1.9650 - val_accuracy: 0.4800 - val_auc: 0.9540\n",
            "Epoch 73/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1513 - accuracy: 0.4332 - auc: 0.9449 - val_loss: 1.9970 - val_accuracy: 0.4750 - val_auc: 0.9525\n",
            "Epoch 74/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1451 - accuracy: 0.4334 - auc: 0.9452 - val_loss: 2.0019 - val_accuracy: 0.4717 - val_auc: 0.9491\n",
            "Epoch 75/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1341 - accuracy: 0.4374 - auc: 0.9456 - val_loss: 2.1898 - val_accuracy: 0.4239 - val_auc: 0.9400\n",
            "Epoch 76/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1209 - accuracy: 0.4390 - auc: 0.9471 - val_loss: 2.0099 - val_accuracy: 0.4673 - val_auc: 0.9505\n",
            "Epoch 77/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1284 - accuracy: 0.4371 - auc: 0.9462 - val_loss: 2.0165 - val_accuracy: 0.4650 - val_auc: 0.9496\n",
            "Epoch 78/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1302 - accuracy: 0.4414 - auc: 0.9454 - val_loss: 2.0013 - val_accuracy: 0.4755 - val_auc: 0.9503\n",
            "Epoch 79/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.1181 - accuracy: 0.4408 - auc: 0.9463 - val_loss: 2.0119 - val_accuracy: 0.4677 - val_auc: 0.9498\n",
            "Epoch 80/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1218 - accuracy: 0.4390 - auc: 0.9465 - val_loss: 1.9822 - val_accuracy: 0.4810 - val_auc: 0.9524\n",
            "Epoch 81/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1171 - accuracy: 0.4376 - auc: 0.9464 - val_loss: 1.9244 - val_accuracy: 0.4926 - val_auc: 0.9537\n",
            "Epoch 82/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1191 - accuracy: 0.4393 - auc: 0.9462 - val_loss: 1.9746 - val_accuracy: 0.4812 - val_auc: 0.9525\n",
            "Epoch 83/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1235 - accuracy: 0.4394 - auc: 0.9462 - val_loss: 1.9564 - val_accuracy: 0.4815 - val_auc: 0.9526\n",
            "Epoch 84/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1195 - accuracy: 0.4365 - auc: 0.9470 - val_loss: 2.0621 - val_accuracy: 0.4619 - val_auc: 0.9479\n",
            "Epoch 85/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1163 - accuracy: 0.4434 - auc: 0.9456 - val_loss: 1.9362 - val_accuracy: 0.4888 - val_auc: 0.9543\n",
            "Epoch 86/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1030 - accuracy: 0.4427 - auc: 0.9471 - val_loss: 2.0934 - val_accuracy: 0.4560 - val_auc: 0.9425\n",
            "Epoch 87/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1014 - accuracy: 0.4448 - auc: 0.9469 - val_loss: 2.1235 - val_accuracy: 0.4484 - val_auc: 0.9412\n",
            "Epoch 88/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1058 - accuracy: 0.4458 - auc: 0.9466 - val_loss: 1.9781 - val_accuracy: 0.4839 - val_auc: 0.9505\n",
            "Epoch 89/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1129 - accuracy: 0.4413 - auc: 0.9456 - val_loss: 1.9592 - val_accuracy: 0.4843 - val_auc: 0.9522\n",
            "Epoch 90/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1022 - accuracy: 0.4450 - auc: 0.9458 - val_loss: 1.9403 - val_accuracy: 0.4866 - val_auc: 0.9532\n",
            "Epoch 91/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.1062 - accuracy: 0.4435 - auc: 0.9464 - val_loss: 1.9822 - val_accuracy: 0.4787 - val_auc: 0.9518\n",
            "\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 92/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0707 - accuracy: 0.4531 - auc: 0.9490 - val_loss: 1.9473 - val_accuracy: 0.4838 - val_auc: 0.9526\n",
            "Epoch 93/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.0484 - accuracy: 0.4605 - auc: 0.9488 - val_loss: 1.8895 - val_accuracy: 0.4999 - val_auc: 0.9552\n",
            "Epoch 94/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0457 - accuracy: 0.4551 - auc: 0.9503 - val_loss: 1.8946 - val_accuracy: 0.4994 - val_auc: 0.9553\n",
            "Epoch 95/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0676 - accuracy: 0.4500 - auc: 0.9492 - val_loss: 1.9061 - val_accuracy: 0.4991 - val_auc: 0.9544\n",
            "Epoch 96/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0554 - accuracy: 0.4547 - auc: 0.9488 - val_loss: 1.9590 - val_accuracy: 0.4834 - val_auc: 0.9506\n",
            "Epoch 97/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0584 - accuracy: 0.4544 - auc: 0.9487 - val_loss: 1.8787 - val_accuracy: 0.4996 - val_auc: 0.9557\n",
            "Epoch 98/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0373 - accuracy: 0.4588 - auc: 0.9505 - val_loss: 1.9553 - val_accuracy: 0.4894 - val_auc: 0.9507\n",
            "Epoch 99/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0435 - accuracy: 0.4577 - auc: 0.9497 - val_loss: 1.8759 - val_accuracy: 0.5046 - val_auc: 0.9559\n",
            "Epoch 100/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.0444 - accuracy: 0.4621 - auc: 0.9485 - val_loss: 1.8590 - val_accuracy: 0.5036 - val_auc: 0.9567\n",
            "Epoch 101/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0211 - accuracy: 0.4647 - auc: 0.9507 - val_loss: 1.8855 - val_accuracy: 0.5003 - val_auc: 0.9551\n",
            "Epoch 102/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0244 - accuracy: 0.4601 - auc: 0.9513 - val_loss: 1.8930 - val_accuracy: 0.4995 - val_auc: 0.9550\n",
            "Epoch 103/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0193 - accuracy: 0.4640 - auc: 0.9507 - val_loss: 1.8892 - val_accuracy: 0.4992 - val_auc: 0.9557\n",
            "Epoch 104/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0422 - accuracy: 0.4567 - auc: 0.9507 - val_loss: 1.9344 - val_accuracy: 0.4888 - val_auc: 0.9528\n",
            "Epoch 105/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0213 - accuracy: 0.4656 - auc: 0.9505 - val_loss: 1.9203 - val_accuracy: 0.4956 - val_auc: 0.9523\n",
            "Epoch 106/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0178 - accuracy: 0.4673 - auc: 0.9501 - val_loss: 1.8767 - val_accuracy: 0.5045 - val_auc: 0.9569\n",
            "Epoch 107/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.0296 - accuracy: 0.4629 - auc: 0.9491 - val_loss: 1.9219 - val_accuracy: 0.4907 - val_auc: 0.9530\n",
            "Epoch 108/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0102 - accuracy: 0.4662 - auc: 0.9513 - val_loss: 1.8527 - val_accuracy: 0.5075 - val_auc: 0.9564\n",
            "Epoch 109/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0115 - accuracy: 0.4613 - auc: 0.9519 - val_loss: 1.9022 - val_accuracy: 0.4994 - val_auc: 0.9539\n",
            "Epoch 110/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0173 - accuracy: 0.4664 - auc: 0.9516 - val_loss: 1.8532 - val_accuracy: 0.5063 - val_auc: 0.9576\n",
            "Epoch 111/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0143 - accuracy: 0.4639 - auc: 0.9511 - val_loss: 1.8776 - val_accuracy: 0.5057 - val_auc: 0.9563\n",
            "Epoch 112/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0079 - accuracy: 0.4631 - auc: 0.9516 - val_loss: 1.8507 - val_accuracy: 0.5046 - val_auc: 0.9572\n",
            "Epoch 113/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.0348 - accuracy: 0.4592 - auc: 0.9500 - val_loss: 1.8983 - val_accuracy: 0.4957 - val_auc: 0.9541\n",
            "Epoch 114/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0038 - accuracy: 0.4688 - auc: 0.9504 - val_loss: 1.8429 - val_accuracy: 0.5103 - val_auc: 0.9579\n",
            "Epoch 115/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0117 - accuracy: 0.4651 - auc: 0.9512 - val_loss: 1.8589 - val_accuracy: 0.5102 - val_auc: 0.9567\n",
            "Epoch 116/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0261 - accuracy: 0.4604 - auc: 0.9515 - val_loss: 1.8620 - val_accuracy: 0.5064 - val_auc: 0.9562\n",
            "Epoch 117/1000\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 2.0092 - accuracy: 0.4664 - auc: 0.9508 - val_loss: 1.8926 - val_accuracy: 0.4999 - val_auc: 0.9536\n",
            "Epoch 118/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.0091 - accuracy: 0.4654 - auc: 0.9518 - val_loss: 1.8547 - val_accuracy: 0.5051 - val_auc: 0.9567\n",
            "Epoch 119/1000\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 2.0034 - accuracy: 0.4666 - auc: 0.9512 - val_loss: 1.8533 - val_accuracy: 0.5104 - val_auc: 0.9565\n",
            "Epoch 120/1000\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 2.0187 - accuracy: 0.4594 - auc: 0.9513"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-34b1914a3d0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_validate\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_train\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     50\u001b[0m \u001b[0mfinish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1139\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1142\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJRHQfLV0fT9",
        "outputId": "a21e2b87-04f0-4640-84da-48e9bd9dee17"
      },
      "source": [
        "# Importing the Keras libraries and packages\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Activation\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "classifier = None\n",
        "classifier = Sequential([\n",
        "    layers.Conv2D(NUM_RECEPTIVE_FILTERS, kernel_size=GABOR_SIZE, strides=(1,1), \n",
        "                  padding='same', name=\"GaborLayer\", input_shape=train_generator.image_shape),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Conv2D(64, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(64, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Conv2D(128, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(128, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    layers.Conv2D(128, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Conv2D(256, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(256, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(256, kernel_size=(3,3), padding='same', strides=(1,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    \n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Dense(512),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Dense(512),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "classifier.summary()\n",
        "\n",
        "import copy\n",
        "untrained_layers = copy.deepcopy(classifier.get_layer(name=classifier.layers[GABOR_LAYER_INDEX].name).get_weights())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "GaborLayer (Conv2D)          (None, 128, 128, 32)      21632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 128, 128, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 64, 64, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 32, 32, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               8389120   \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               51300     \n",
            "=================================================================\n",
            "Total params: 10,643,204\n",
            "Trainable params: 10,638,468\n",
            "Non-trainable params: 4,736\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NzxrvzG8aXE",
        "outputId": "4f08306a-4d44-4af6-f4dc-ff8d6172b936"
      },
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,  \n",
        "                              min_delta=1e-4, mode='min', verbose=1)\n",
        "stop_alg = EarlyStopping(monitor='val_loss', patience=35, \n",
        "                         restore_best_weights=True, verbose=1)\n",
        "callbacks = [stop_alg, reduce_lr]\n",
        "opt = Adam(learning_rate=0.001)\n",
        "classifier.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', 'AUC'])\n",
        "\n",
        "start = time.perf_counter()\n",
        "hist = classifier.fit(\n",
        "    train_generator, \n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=total_validate//BATCH_SIZE,\n",
        "    steps_per_epoch=total_train//BATCH_SIZE,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "finish = time.perf_counter()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1562/1562 [==============================] - 83s 52ms/step - loss: 3.9845 - accuracy: 0.1011 - auc: 0.7641 - val_loss: 3.1555 - val_accuracy: 0.2346 - val_auc: 0.8731\n",
            "Epoch 2/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 2.7332 - accuracy: 0.3118 - auc: 0.9122 - val_loss: 2.6687 - val_accuracy: 0.3343 - val_auc: 0.9071\n",
            "Epoch 3/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 2.1545 - accuracy: 0.4331 - auc: 0.9456 - val_loss: 2.4470 - val_accuracy: 0.3814 - val_auc: 0.9168\n",
            "Epoch 4/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 1.8350 - accuracy: 0.5000 - auc: 0.9605 - val_loss: 1.9070 - val_accuracy: 0.4875 - val_auc: 0.9525\n",
            "Epoch 5/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 1.5858 - accuracy: 0.5588 - auc: 0.9695 - val_loss: 2.2772 - val_accuracy: 0.4279 - val_auc: 0.9244\n",
            "Epoch 6/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 1.3543 - accuracy: 0.6147 - auc: 0.9774 - val_loss: 1.8075 - val_accuracy: 0.5154 - val_auc: 0.9528\n",
            "Epoch 7/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 1.1945 - accuracy: 0.6596 - auc: 0.9819 - val_loss: 1.8798 - val_accuracy: 0.5130 - val_auc: 0.9449\n",
            "Epoch 8/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 1.0218 - accuracy: 0.7036 - auc: 0.9856 - val_loss: 1.6737 - val_accuracy: 0.5680 - val_auc: 0.9517\n",
            "Epoch 9/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.8803 - accuracy: 0.7394 - auc: 0.9891 - val_loss: 1.7924 - val_accuracy: 0.5593 - val_auc: 0.9423\n",
            "Epoch 10/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.7565 - accuracy: 0.7724 - auc: 0.9915 - val_loss: 1.9342 - val_accuracy: 0.5415 - val_auc: 0.9323\n",
            "Epoch 11/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.6385 - accuracy: 0.8082 - auc: 0.9936 - val_loss: 1.8847 - val_accuracy: 0.5557 - val_auc: 0.9350\n",
            "Epoch 12/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.5502 - accuracy: 0.8345 - auc: 0.9941 - val_loss: 1.8778 - val_accuracy: 0.5725 - val_auc: 0.9317\n",
            "Epoch 13/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.4743 - accuracy: 0.8550 - auc: 0.9954 - val_loss: 1.9151 - val_accuracy: 0.5761 - val_auc: 0.9308\n",
            "Epoch 14/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.4030 - accuracy: 0.8737 - auc: 0.9966 - val_loss: 1.9756 - val_accuracy: 0.5800 - val_auc: 0.9237\n",
            "Epoch 15/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.3739 - accuracy: 0.8812 - auc: 0.9973 - val_loss: 2.1453 - val_accuracy: 0.5616 - val_auc: 0.9178\n",
            "Epoch 16/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.3234 - accuracy: 0.8985 - auc: 0.9972 - val_loss: 2.0710 - val_accuracy: 0.5805 - val_auc: 0.9181\n",
            "Epoch 17/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.3077 - accuracy: 0.9007 - auc: 0.9976 - val_loss: 2.0927 - val_accuracy: 0.5781 - val_auc: 0.9194\n",
            "Epoch 18/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.2765 - accuracy: 0.9098 - auc: 0.9976 - val_loss: 2.2548 - val_accuracy: 0.5641 - val_auc: 0.9097\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 19/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.1873 - accuracy: 0.9402 - auc: 0.9989 - val_loss: 2.0032 - val_accuracy: 0.6061 - val_auc: 0.9212\n",
            "Epoch 20/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.1363 - accuracy: 0.9563 - auc: 0.9993 - val_loss: 2.0625 - val_accuracy: 0.6062 - val_auc: 0.9182\n",
            "Epoch 21/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.1170 - accuracy: 0.9638 - auc: 0.9994 - val_loss: 2.2022 - val_accuracy: 0.5931 - val_auc: 0.9121\n",
            "Epoch 22/1000\n",
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.1079 - accuracy: 0.9653 - auc: 0.9995 - val_loss: 2.1789 - val_accuracy: 0.6064 - val_auc: 0.9133\n",
            "Epoch 23/1000\n",
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.0921 - accuracy: 0.9711 - auc: 0.9996 - val_loss: 2.3274 - val_accuracy: 0.5956 - val_auc: 0.9055\n",
            "Epoch 24/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.0951 - accuracy: 0.9683 - auc: 0.9996 - val_loss: 2.2572 - val_accuracy: 0.6087 - val_auc: 0.9102\n",
            "Epoch 25/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.0868 - accuracy: 0.9711 - auc: 0.9996 - val_loss: 2.3200 - val_accuracy: 0.5990 - val_auc: 0.9079\n",
            "Epoch 26/1000\n",
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.0856 - accuracy: 0.9726 - auc: 0.9994 - val_loss: 2.4142 - val_accuracy: 0.6029 - val_auc: 0.9028\n",
            "Epoch 27/1000\n",
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.0786 - accuracy: 0.9747 - auc: 0.9995 - val_loss: 2.4027 - val_accuracy: 0.6017 - val_auc: 0.9024\n",
            "Epoch 28/1000\n",
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.0749 - accuracy: 0.9758 - auc: 0.9996 - val_loss: 2.4963 - val_accuracy: 0.6030 - val_auc: 0.8986\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 29/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.0626 - accuracy: 0.9800 - auc: 0.9997 - val_loss: 2.3449 - val_accuracy: 0.6145 - val_auc: 0.9049\n",
            "Epoch 30/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.0471 - accuracy: 0.9854 - auc: 0.9997 - val_loss: 2.3396 - val_accuracy: 0.6186 - val_auc: 0.9048\n",
            "Epoch 31/1000\n",
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.0449 - accuracy: 0.9860 - auc: 0.9998 - val_loss: 2.3775 - val_accuracy: 0.6151 - val_auc: 0.9051\n",
            "Epoch 32/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.0419 - accuracy: 0.9869 - auc: 0.9998 - val_loss: 2.4208 - val_accuracy: 0.6137 - val_auc: 0.9039\n",
            "Epoch 33/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.0361 - accuracy: 0.9891 - auc: 0.9998 - val_loss: 2.3946 - val_accuracy: 0.6177 - val_auc: 0.9037\n",
            "Epoch 34/1000\n",
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.0355 - accuracy: 0.9895 - auc: 0.9998 - val_loss: 2.4275 - val_accuracy: 0.6189 - val_auc: 0.9034\n",
            "Epoch 35/1000\n",
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.0340 - accuracy: 0.9894 - auc: 0.9999 - val_loss: 2.4197 - val_accuracy: 0.6162 - val_auc: 0.9028\n",
            "Epoch 36/1000\n",
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.0319 - accuracy: 0.9902 - auc: 0.9999 - val_loss: 2.4320 - val_accuracy: 0.6138 - val_auc: 0.9037\n",
            "Epoch 37/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.0316 - accuracy: 0.9899 - auc: 0.9999 - val_loss: 2.4360 - val_accuracy: 0.6158 - val_auc: 0.9021\n",
            "Epoch 38/1000\n",
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.0315 - accuracy: 0.9901 - auc: 0.9999 - val_loss: 2.5303 - val_accuracy: 0.6150 - val_auc: 0.8994\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 39/1000\n",
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.0274 - accuracy: 0.9918 - auc: 0.9998 - val_loss: 2.4312 - val_accuracy: 0.6235 - val_auc: 0.9048\n",
            "Epoch 40/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.0228 - accuracy: 0.9929 - auc: 0.9999 - val_loss: 2.4115 - val_accuracy: 0.6234 - val_auc: 0.9049\n",
            "Epoch 41/1000\n",
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.0206 - accuracy: 0.9937 - auc: 1.0000 - val_loss: 2.4471 - val_accuracy: 0.6231 - val_auc: 0.9038\n",
            "Epoch 42/1000\n",
            "1562/1562 [==============================] - 81s 52ms/step - loss: 0.0202 - accuracy: 0.9932 - auc: 1.0000 - val_loss: 2.4118 - val_accuracy: 0.6243 - val_auc: 0.9041\n",
            "Epoch 43/1000\n",
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.0182 - accuracy: 0.9941 - auc: 1.0000 - val_loss: 2.4627 - val_accuracy: 0.6225 - val_auc: 0.9027\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00043: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wYm50V4v0i9E",
        "outputId": "890d91da-162f-49f3-b055-a18ab27bd68f"
      },
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,  \n",
        "                              min_delta=1e-4, mode='min', verbose=1)\n",
        "stop_alg = EarlyStopping(monitor='val_loss', patience=35, \n",
        "                         restore_best_weights=True, verbose=1)\n",
        "callbacks = [stop_alg, reduce_lr]\n",
        "opt = Adam(learning_rate=0.001)\n",
        "classifier.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', 'AUC'])\n",
        "\n",
        "start = time.perf_counter()\n",
        "hist = classifier.fit(\n",
        "    train_generator, \n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=total_validate//BATCH_SIZE,\n",
        "    steps_per_epoch=total_train//BATCH_SIZE,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "finish = time.perf_counter()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "312/312 [==============================] - 57s 166ms/step - loss: 4.5369 - accuracy: 0.0324 - auc: 0.6455 - val_loss: 5.1287 - val_accuracy: 0.0531 - val_auc: 0.6710\n",
            "Epoch 2/1000\n",
            "312/312 [==============================] - 50s 161ms/step - loss: 3.8352 - accuracy: 0.1162 - auc: 0.8035 - val_loss: 5.7407 - val_accuracy: 0.0761 - val_auc: 0.6485\n",
            "Epoch 3/1000\n",
            "312/312 [==============================] - 50s 161ms/step - loss: 3.3564 - accuracy: 0.1931 - auc: 0.8635 - val_loss: 6.5729 - val_accuracy: 0.0660 - val_auc: 0.6249\n",
            "Epoch 4/1000\n",
            "312/312 [==============================] - 50s 160ms/step - loss: 2.9144 - accuracy: 0.2709 - auc: 0.9053 - val_loss: 4.3447 - val_accuracy: 0.1520 - val_auc: 0.7564\n",
            "Epoch 5/1000\n",
            "312/312 [==============================] - 50s 161ms/step - loss: 2.4667 - accuracy: 0.3673 - auc: 0.9357 - val_loss: 3.9080 - val_accuracy: 0.1878 - val_auc: 0.7990\n",
            "Epoch 6/1000\n",
            "312/312 [==============================] - 50s 161ms/step - loss: 1.9675 - accuracy: 0.4828 - auc: 0.9637 - val_loss: 3.8244 - val_accuracy: 0.2027 - val_auc: 0.8097\n",
            "Epoch 7/1000\n",
            "312/312 [==============================] - 50s 161ms/step - loss: 1.4534 - accuracy: 0.6152 - auc: 0.9813 - val_loss: 5.2655 - val_accuracy: 0.1447 - val_auc: 0.7176\n",
            "Epoch 8/1000\n",
            "312/312 [==============================] - 50s 160ms/step - loss: 0.9346 - accuracy: 0.7560 - auc: 0.9937 - val_loss: 3.7669 - val_accuracy: 0.2481 - val_auc: 0.8164\n",
            "Epoch 9/1000\n",
            "312/312 [==============================] - 49s 158ms/step - loss: 0.5671 - accuracy: 0.8633 - auc: 0.9977 - val_loss: 3.7037 - val_accuracy: 0.2731 - val_auc: 0.8204\n",
            "Epoch 10/1000\n",
            "312/312 [==============================] - 49s 159ms/step - loss: 0.3303 - accuracy: 0.9219 - auc: 0.9994 - val_loss: 4.0111 - val_accuracy: 0.2569 - val_auc: 0.8028\n",
            "Epoch 11/1000\n",
            "312/312 [==============================] - 49s 159ms/step - loss: 0.2061 - accuracy: 0.9563 - auc: 0.9996 - val_loss: 4.1197 - val_accuracy: 0.2559 - val_auc: 0.7973\n",
            "Epoch 12/1000\n",
            "312/312 [==============================] - 49s 158ms/step - loss: 0.1610 - accuracy: 0.9628 - auc: 0.9999 - val_loss: 4.4821 - val_accuracy: 0.2481 - val_auc: 0.7780\n",
            "Epoch 13/1000\n",
            "312/312 [==============================] - 50s 161ms/step - loss: 0.1177 - accuracy: 0.9742 - auc: 0.9999 - val_loss: 4.4343 - val_accuracy: 0.2570 - val_auc: 0.7820\n",
            "Epoch 14/1000\n",
            "312/312 [==============================] - 49s 159ms/step - loss: 0.1291 - accuracy: 0.9675 - auc: 0.9998 - val_loss: 4.7949 - val_accuracy: 0.2448 - val_auc: 0.7628\n",
            "Epoch 15/1000\n",
            "312/312 [==============================] - 50s 159ms/step - loss: 0.1543 - accuracy: 0.9559 - auc: 0.9997 - val_loss: 4.9003 - val_accuracy: 0.2510 - val_auc: 0.7639\n",
            "Epoch 16/1000\n",
            "312/312 [==============================] - 49s 159ms/step - loss: 0.1467 - accuracy: 0.9567 - auc: 0.9994 - val_loss: 5.1782 - val_accuracy: 0.2470 - val_auc: 0.7511\n",
            "Epoch 17/1000\n",
            "312/312 [==============================] - 49s 158ms/step - loss: 0.1632 - accuracy: 0.9511 - auc: 0.9994 - val_loss: 5.2780 - val_accuracy: 0.2401 - val_auc: 0.7467\n",
            "Epoch 18/1000\n",
            "312/312 [==============================] - 49s 159ms/step - loss: 0.1388 - accuracy: 0.9605 - auc: 0.9997 - val_loss: 5.1434 - val_accuracy: 0.2511 - val_auc: 0.7557\n",
            "Epoch 19/1000\n",
            "312/312 [==============================] - 50s 160ms/step - loss: 0.1081 - accuracy: 0.9684 - auc: 0.9993 - val_loss: 5.0512 - val_accuracy: 0.2627 - val_auc: 0.7607\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 20/1000\n",
            "312/312 [==============================] - 50s 159ms/step - loss: 0.0593 - accuracy: 0.9843 - auc: 0.9996 - val_loss: 4.6402 - val_accuracy: 0.2881 - val_auc: 0.7798\n",
            "Epoch 21/1000\n",
            "312/312 [==============================] - 50s 160ms/step - loss: 0.0198 - accuracy: 0.9969 - auc: 0.9999 - val_loss: 4.6653 - val_accuracy: 0.2885 - val_auc: 0.7784\n",
            "Epoch 22/1000\n",
            "312/312 [==============================] - 50s 160ms/step - loss: 0.0164 - accuracy: 0.9971 - auc: 1.0000 - val_loss: 4.6534 - val_accuracy: 0.2903 - val_auc: 0.7794\n",
            "Epoch 23/1000\n",
            "312/312 [==============================] - 49s 159ms/step - loss: 0.0143 - accuracy: 0.9967 - auc: 1.0000 - val_loss: 4.6654 - val_accuracy: 0.2924 - val_auc: 0.7789\n",
            "Epoch 24/1000\n",
            "312/312 [==============================] - 50s 160ms/step - loss: 0.0104 - accuracy: 0.9984 - auc: 1.0000 - val_loss: 4.6883 - val_accuracy: 0.2932 - val_auc: 0.7779\n",
            "Epoch 25/1000\n",
            "312/312 [==============================] - 50s 159ms/step - loss: 0.0147 - accuracy: 0.9972 - auc: 0.9999 - val_loss: 4.7973 - val_accuracy: 0.2891 - val_auc: 0.7743\n",
            "Epoch 26/1000\n",
            "312/312 [==============================] - 50s 159ms/step - loss: 0.0150 - accuracy: 0.9965 - auc: 1.0000 - val_loss: 4.8727 - val_accuracy: 0.2833 - val_auc: 0.7696\n",
            "Epoch 27/1000\n",
            "312/312 [==============================] - 49s 159ms/step - loss: 0.0120 - accuracy: 0.9979 - auc: 0.9999 - val_loss: 5.0239 - val_accuracy: 0.2759 - val_auc: 0.7637\n",
            "Epoch 28/1000\n",
            "312/312 [==============================] - 50s 159ms/step - loss: 0.0191 - accuracy: 0.9958 - auc: 0.9999 - val_loss: 5.1687 - val_accuracy: 0.2779 - val_auc: 0.7602\n",
            "Epoch 29/1000\n",
            "312/312 [==============================] - 50s 159ms/step - loss: 0.0166 - accuracy: 0.9962 - auc: 1.0000 - val_loss: 5.1736 - val_accuracy: 0.2752 - val_auc: 0.7584\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 30/1000\n",
            "312/312 [==============================] - 50s 159ms/step - loss: 0.0121 - accuracy: 0.9977 - auc: 1.0000 - val_loss: 4.9354 - val_accuracy: 0.2858 - val_auc: 0.7681\n",
            "Epoch 31/1000\n",
            "312/312 [==============================] - 50s 160ms/step - loss: 0.0156 - accuracy: 0.9958 - auc: 0.9998 - val_loss: 5.0037 - val_accuracy: 0.2856 - val_auc: 0.7662\n",
            "Epoch 32/1000\n",
            "312/312 [==============================] - 50s 160ms/step - loss: 0.0071 - accuracy: 0.9986 - auc: 1.0000 - val_loss: 5.0354 - val_accuracy: 0.2861 - val_auc: 0.7649\n",
            "Epoch 33/1000\n",
            "312/312 [==============================] - 50s 160ms/step - loss: 0.0056 - accuracy: 0.9991 - auc: 1.0000 - val_loss: 4.9393 - val_accuracy: 0.2897 - val_auc: 0.7690\n",
            "Epoch 34/1000\n",
            "312/312 [==============================] - 50s 159ms/step - loss: 0.0048 - accuracy: 0.9992 - auc: 1.0000 - val_loss: 4.9386 - val_accuracy: 0.2931 - val_auc: 0.7704\n",
            "Epoch 35/1000\n",
            "312/312 [==============================] - 49s 159ms/step - loss: 0.0048 - accuracy: 0.9992 - auc: 1.0000 - val_loss: 5.2031 - val_accuracy: 0.2773 - val_auc: 0.7581\n",
            "Epoch 36/1000\n",
            "312/312 [==============================] - 49s 159ms/step - loss: 0.0051 - accuracy: 0.9989 - auc: 1.0000 - val_loss: 4.9763 - val_accuracy: 0.2927 - val_auc: 0.7695\n",
            "Epoch 37/1000\n",
            "312/312 [==============================] - 49s 159ms/step - loss: 0.0035 - accuracy: 0.9994 - auc: 1.0000 - val_loss: 4.9717 - val_accuracy: 0.2923 - val_auc: 0.7689\n",
            "Epoch 38/1000\n",
            "312/312 [==============================] - 50s 161ms/step - loss: 0.0027 - accuracy: 0.9998 - auc: 1.0000 - val_loss: 5.0172 - val_accuracy: 0.2930 - val_auc: 0.7666\n",
            "Epoch 39/1000\n",
            "312/312 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9995 - auc: 1.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-98c01fd43a63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_validate\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_train\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[0mfinish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1139\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1142\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPd59y7J0rdp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}