{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "caltech-resnet50.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNic8Xb6ohBdFMOeadn6z+X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehang/Improvised-CNN/blob/master/notebook/Caltech-256-128/resized/15*15/caltech_resnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oqau9hDnAD4R",
        "outputId": "949820dc-594a-491a-b041-0f11573eec54"
      },
      "source": [
        "print(\"jpt\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MveDm4tt__bk"
      },
      "source": [
        "# Importing the Keras libraries and packages\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Activation\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Basic ResNet Building Block \n",
        "def resnet_layer(inputs, num_filters = 16, kernel_size = 3, strides = 1, \n",
        "                 activation ='relu', batch_normalization = True, \n",
        "    conv = Conv2D(num_filters, kernel_size = kernel_size, strides = strides, \n",
        "                  padding ='same', kernel_initializer ='he_normal', \n",
        "                  kernel_regularizer = l2(1e-4)) \n",
        "  \n",
        "    x = inputs \n",
        "    if conv_first: \n",
        "        x = conv(x) \n",
        "        if batch_normalization: \n",
        "            x = BatchNormalization()(x) \n",
        "        if activation is not None: \n",
        "            x = Activation(activation)(x) \n",
        "    else: \n",
        "        if batch_normalization: \n",
        "            x = BatchNormalization()(x) \n",
        "        if activation is not None: \n",
        "            x = Activation(activation)(x) \n",
        "        x = conv(x) \n",
        "    return x\n",
        "\n",
        "def _residual_block_first(self, x, out_channel, strides, name=\"unit\"):\n",
        "    in_channel = x.get_shape().as_list()[-1]\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        print('\\tBuilding residual unit: %s' % scope.name)\n",
        "\n",
        "        # Shortcut connection\n",
        "        if in_channel == out_channel:\n",
        "            if strides == 1:\n",
        "                shortcut = tf.identity(x)\n",
        "            else:\n",
        "                shortcut = tf.nn.max_pool(x, [1, strides, strides, 1], [1, strides, strides, 1], 'VALID')\n",
        "        else:\n",
        "            shortcut = self._conv(x, 1, out_channel, strides, name='shortcut')\n",
        "        # Residual\n",
        "        x = self._conv(x, 3, out_channel, strides, name='conv_1')\n",
        "        x = self._bn(x, name='bn_1')\n",
        "        x = self._relu(x, name='relu_1')\n",
        "        x = self._conv(x, 3, out_channel, 1, name='conv_2')\n",
        "        x = self._bn(x, name='bn_2')\n",
        "        # Merge\n",
        "        x = x + shortcut\n",
        "        x = self._relu(x, name='relu_2')\n",
        "    return x\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes = 10): \n",
        "      \n",
        "    if (depth - 2) % 6 != 0: \n",
        "        raise ValueError('depth should be 6n + 2 (eg 20, 32, 44 in [a])') \n",
        "    # Start model definition. \n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6) \n",
        "  \n",
        "    inputs = Input(shape = input_shape) \n",
        "    x = resnet_layer(inputs = inputs) \n",
        "    # Instantiate the stack of residual units \n",
        "    for stack in range(3): \n",
        "        for res_block in range(num_res_blocks): \n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack \n",
        "                strides = 2  # downsample \n",
        "            y = resnet_layer(inputs = x, \n",
        "                             num_filters = num_filters, \n",
        "                             strides = strides) \n",
        "            y = resnet_layer(inputs = y, \n",
        "                             num_filters = num_filters, \n",
        "                             activation = None) \n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack \n",
        "                # linear projection residual shortcut connection to match \n",
        "                # changed dims \n",
        "                x = resnet_layer(inputs = x, \n",
        "                                 num_filters = num_filters, \n",
        "                                 kernel_size = 1, \n",
        "                                 strides = strides, \n",
        "                                 activation = None, \n",
        "                                 batch_normalization = False) \n",
        "            x = keras.layers.add([x, y]) \n",
        "            x = Activation('relu')(x) \n",
        "        num_filters *= 2\n",
        "  \n",
        "    # Add classifier on top. \n",
        "    # v1 does not use BN after last shortcut connection-ReLU \n",
        "    x = AveragePooling2D(pool_size = 8)(x) \n",
        "    y = Flatten()(x) \n",
        "    outputs = Dense(num_classes, \n",
        "                    activation ='softmax', \n",
        "                    kernel_initializer ='he_normal')(y) \n",
        "  \n",
        "    # Instantiate model. \n",
        "    model = Model(inputs = inputs, outputs = outputs) \n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbjsLjmsJGJN"
      },
      "source": [
        "\n",
        "def res_identity(x, filters): \n",
        "  #resnet block where dimension does not change.\n",
        "  #The skip connection is just simple identity conncection\n",
        "  #we will have 3 blocks and then input will be added\n",
        "\n",
        "  x_skip = x # this will be used for addition with the residual block \n",
        "  f1, f2 = filters\n",
        "\n",
        "  #first block \n",
        "  x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation(activations.relu)(x)\n",
        "\n",
        "  #second block # bottleneck (but size kept same with padding)\n",
        "  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation(activations.relu)(x)\n",
        "\n",
        "  # third block activation used after adding the input\n",
        "  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  # x = Activation(activations.relu)(x)\n",
        "\n",
        "  # add the input \n",
        "  x = Add()([x, x_skip])\n",
        "  x = Activation(activations.relu)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def res_conv(x, s, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
        "  # when s = 2 then it is like downsizing the feature map\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation(activations.relu)(x)\n",
        "\n",
        "  # second block\n",
        "  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation(activations.relu)(x)\n",
        "\n",
        "  #third block\n",
        "  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # shortcut \n",
        "  x_skip = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(x_skip)\n",
        "  x_skip = BatchNormalization()(x_skip)\n",
        "\n",
        "  # add \n",
        "  x = Add()([x, x_skip])\n",
        "  x = Activation(activations.relu)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def resnet50():\n",
        "\n",
        "  input_im = Input(shape=(train_im.shape[1], train_im.shape[2], train_im.shape[3])) # cifar 10 images size\n",
        "  x = ZeroPadding2D(padding=(3, 3))(input_im)\n",
        "\n",
        "  # 1st stage\n",
        "  # here we perform maxpooling, see the figure above\n",
        "\n",
        "  x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation(activations.relu)(x)\n",
        "  x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "  #2nd stage \n",
        "  # frm here on only conv block and identity block, no pooling\n",
        "\n",
        "  x = res_conv(x, s=1, filters=(64, 256))\n",
        "  x = res_identity(x, filters=(64, 256))\n",
        "  x = res_identity(x, filters=(64, 256))\n",
        "\n",
        "  # 3rd stage\n",
        "\n",
        "  x = res_conv(x, s=2, filters=(128, 512))\n",
        "  x = res_identity(x, filters=(128, 512))\n",
        "  x = res_identity(x, filters=(128, 512))\n",
        "  x = res_identity(x, filters=(128, 512))\n",
        "\n",
        "  # 4th stage\n",
        "\n",
        "  x = res_conv(x, s=2, filters=(256, 1024))\n",
        "  x = res_identity(x, filters=(256, 1024))\n",
        "  x = res_identity(x, filters=(256, 1024))\n",
        "  x = res_identity(x, filters=(256, 1024))\n",
        "  x = res_identity(x, filters=(256, 1024))\n",
        "  x = res_identity(x, filters=(256, 1024))\n",
        "\n",
        "  # 5th stage\n",
        "\n",
        "  x = res_conv(x, s=2, filters=(512, 2048))\n",
        "  x = res_identity(x, filters=(512, 2048))\n",
        "  x = res_identity(x, filters=(512, 2048))\n",
        "\n",
        "  # ends with average pooling and dense connection\n",
        "\n",
        "  x = AveragePooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(len(class_types), activation='softmax', kernel_initializer='he_normal')(x) #multi-class\n",
        "\n",
        "  # define the model \n",
        "\n",
        "  model = Model(inputs=input_im, outputs=x, name='Resnet50')\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}