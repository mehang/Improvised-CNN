{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"../../../../../dataset/all/\")\n",
    "datapath = \"../../../../../dataset/all/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST_RUN = False\n",
    "IMAGE_WIDTH=256\n",
    "IMAGE_HEIGHT=256\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_CHANNELS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 validated image filenames belonging to 2 classes.\n",
      "Found 5000 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/1000\n",
      "235/625 [==========>...................] - ETA: 29:11 - loss: 0.8600 - accuracy: 0.4944"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-504e18da2899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_validate\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_train\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m     \u001b[0mfinish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PersonalCode/thesis/cnn-env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PersonalCode/thesis/cnn-env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PersonalCode/thesis/cnn-env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PersonalCode/thesis/cnn-env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PersonalCode/thesis/cnn-env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PersonalCode/thesis/cnn-env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PersonalCode/thesis/cnn-env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PersonalCode/thesis/cnn-env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/PersonalCode/thesis/cnn-env/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "normal_history = []\n",
    "gabor_history = []\n",
    "# Importing the Keras libraries and packages\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Activation\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "for i in range(10):\n",
    "    filenames = os.listdir(datapath)\n",
    "    categories = []\n",
    "    for filename in filenames:\n",
    "        category = filename.split('.')[0]\n",
    "        if category == 'dog':\n",
    "            categories.append(1)\n",
    "        else:\n",
    "            categories.append(0)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'filename': filenames,\n",
    "        'category': categories\n",
    "    })\n",
    "    df[\"category\"] = df[\"category\"].replace({0: 'cat', 1: 'dog'}) \n",
    "    train_df, validate_df = train_test_split(df, test_size=0.20) #, random_state=42)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    validate_df = validate_df.reset_index(drop=True)\n",
    "    total_train = train_df.shape[0]\n",
    "    total_validate = validate_df.shape[0]\n",
    "    batch_size=32\n",
    "    train_datagen = None\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "    )\n",
    "    train_generator = None\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        train_df, \n",
    "        datapath, \n",
    "        x_col='filename',\n",
    "        y_col='category',\n",
    "        target_size=IMAGE_SIZE,\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    validation_datagen = None\n",
    "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    validation_generator = None\n",
    "    validation_generator = validation_datagen.flow_from_dataframe(\n",
    "        validate_df, \n",
    "        datapath, \n",
    "        x_col='filename',\n",
    "        y_col='category',\n",
    "        target_size=IMAGE_SIZE,\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    num_classes = 2\n",
    "    classifier = None\n",
    "    classifier = Sequential([\n",
    "        # layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_size, img_size, 3)),\n",
    "        layers.Conv2D(32, kernel_size=(15,15), strides=(1,1), activation='relu', input_shape=(256, 256, 3)),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        Dropout(0.5),\n",
    "        layers.Conv2D(64, kernel_size=(3,3), strides=(2,2), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        Dropout(0.5),\n",
    "        layers.Conv2D(128, kernel_size=(3,3), strides=(2,2), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        Dropout(0.5),\n",
    "        layers.Conv2D(128, kernel_size=(3,3), strides=(2,2), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        layers.Dense(num_classes)\n",
    "    ])\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,  \n",
    "                                  min_delta=1e-4, mode='min', verbose=1)\n",
    "    stop_alg = EarlyStopping(monitor='val_loss', patience=35, \n",
    "                             restore_best_weights=True, verbose=1)\n",
    "    callbacks = [stop_alg, reduce_lr]\n",
    "    epochs=20 if FAST_RUN else 1000\n",
    "    start = time.perf_counter()\n",
    "    classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    hist = None\n",
    "    hist = classifier.fit(\n",
    "        train_generator, \n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=total_validate//batch_size,\n",
    "        steps_per_epoch=total_train//batch_size,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    finish = time.perf_counter()\n",
    "    print(\"Normal learning iteration {} : start-{} finish-{} training time-{}\".format(i,start, finish, finish-start))\n",
    "    \n",
    "    normal_history.append(hist)\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    plt.plot(hist.history['loss'], color='#785ef0')\n",
    "    plt.plot(hist.history['val_loss'], color='#dc267f')\n",
    "    plt.title('Model Loss Progress')\n",
    "    plt.ylabel('Binary Cross-Entropy Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training Set', 'Test Set'], loc='upper right')\n",
    "    plt.savefig('Iteration-{}-normal-loss.png'.format(i), dpi=350, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    plt.plot(hist.history['accuracy'], color='#785ef0')\n",
    "    plt.plot(hist.history['val_accuracy'], color='#dc267f')\n",
    "    plt.title('Model Loss Progress')\n",
    "    plt.ylabel('Binary Cross-Entropy Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training Set', 'Test Set'], loc='upper right')\n",
    "    plt.savefig('Iteration-{}-normal-accuracy.png'.format(i), dpi=350, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    cnnl1 = classifier.layers[0].name   # get the name of the first conv layer\n",
    "    W = classifier.get_layer(name=cnnl1).get_weights()[0]   #get the filters\n",
    "    wshape = W.shape  #save the original shape\n",
    "    # this part will scale to [0, 1] for visualization purposes\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(W.reshape(-1,1))\n",
    "    W = scaler.transform(W.reshape(-1,1))\n",
    "    W = W.reshape(wshape)\n",
    "    # since there are 32 filters, we will display them 8x4\n",
    "    fig, axs = plt.subplots(8,4, figsize=(24,24))\n",
    "    fig.subplots_adjust(hspace = .25, wspace=.001)\n",
    "    axs = axs.ravel()\n",
    "    for i in range(W.shape[-1]):\n",
    "      # we reshape to a 3D (RGB) image shape and display\n",
    "      h = np.reshape(W[:,:,:,i], (15,15,3))\n",
    "      axs[i].imshow(h)\n",
    "      axs[i].set_title('Filter ' + str(i))    \n",
    "    plt.savefig('Iteration-{}-normal-trainedFilters.png'.format(i), bbox_inches='tight', dpi=350)\n",
    "\n",
    "    \n",
    "    num_classes = 2\n",
    "    classifier = None\n",
    "    classifier = Sequential([\n",
    "        # layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_size, img_size, 3)),\n",
    "        layers.Conv2D(32, kernel_size=(15,15), strides=(1,1), activation='relu', input_shape=(256, 256, 3)),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        Dropout(0.5),\n",
    "        layers.Conv2D(64, kernel_size=(3,3), strides=(2,2), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        Dropout(0.5),\n",
    "        layers.Conv2D(128, kernel_size=(3,3), strides=(2,2), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        Dropout(0.5),\n",
    "        layers.Conv2D(128, kernel_size=(3,3), strides=(2,2), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        layers.Dense(num_classes)\n",
    "    ])\n",
    "    def get_gabor_filters(inchannels, outchannels, kernel_size = (3,3)):\n",
    "        delta = 1e-4\n",
    "        freqs = (math.pi/2)*(math.sqrt(2)**(-np.random.randint(0,5, (outchannels, inchannels))))\n",
    "        thetas = (math.pi/8)*np.random.randint(0,8, (outchannels, inchannels))\n",
    "        sigmas = math.pi/(freqs)\n",
    "        psis = math.pi * np.random.rand(outchannels, inchannels)\n",
    "        x0, y0 = np.ceil(np.array(kernel_size)/2)    \n",
    "\n",
    "        y, x = np.meshgrid(\n",
    "                np.linspace(-x0 + 1, x0 + 0, kernel_size[0]),\n",
    "                np.linspace(-y0 + 1, y0 + 0, kernel_size[1]),\n",
    "        )\n",
    "        filterbank = []\n",
    "\n",
    "        for i in range(outchannels):\n",
    "            for j in range(inchannels):\n",
    "                freq = freqs[i][j]\n",
    "                theta = thetas[i][j]\n",
    "                sigma = sigmas[i][j]\n",
    "                psi = psis[i][j]\n",
    "\n",
    "                rotx = x * np.cos(theta) + y * np.sin(theta)\n",
    "                roty = -x * np.sin(theta) + y * np.cos(theta)\n",
    "\n",
    "                g = np.exp(\n",
    "                    -0.5 * ((rotx ** 2 + roty ** 2) / (sigma + delta) ** 2)\n",
    "                )\n",
    "                g = g * np.cos(freq * rotx + psi)\n",
    "    #             g = g / (2 * math.pi * (sigma ** 2))\n",
    "    #             g = gabor_kernel(frequency=freq, bandwidth=sigma, theta=theta, offset=psi, n_stds=0).real\n",
    "                filterbank.append(g)\n",
    "        return filterbank\n",
    "\n",
    "    filterbank = get_gabor_filters(3, 32, (15,15))\n",
    "\n",
    "    fig = plt.subplots(8, len(filterbank)//8, figsize=(22,22))\n",
    "    for i,gf in enumerate(filterbank):\n",
    "        plt.subplot(8, len(filterbank)//8, i+1)\n",
    "        plt.imshow(gf, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    cnnl1 = classifier.layers[0].name   # get the name of the first conv layer\n",
    "    W = classifier.get_layer(name=cnnl1).get_weights()[0]   #get the filters\n",
    "    wshape = W.shape  #save the original shape\n",
    "    gabor_filters = W\n",
    "    for kernel_index in range(wshape[3]):\n",
    "        for channel_index in range(3):\n",
    "            gabor_filters[:,:,channel_index, kernel_index] = filterbank[kernel_index+channel_index]\n",
    "\n",
    "    classifier.get_layer(name=cnnl1).set_weights([gabor_filters, classifier.get_layer(name=cnnl1).get_weights()[1]])\n",
    "    filter_layers = []\n",
    "    for i in range(32):\n",
    "        for j in range(3):\n",
    "            filter_layers.append(np.reshape(W[:,:,j, i], (15,15)))\n",
    "    for i,gf in enumerate(filter_layers):\n",
    "        plt.subplot(8, len(filterbank)//8, i+1)\n",
    "        plt.imshow(gf, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,  \n",
    "                                  min_delta=1e-4, mode='min', verbose=1)\n",
    "\n",
    "    stop_alg = EarlyStopping(monitor='val_loss', patience=35, \n",
    "                             restore_best_weights=True, verbose=1)\n",
    "\n",
    "    callbacks = [stop_alg, reduce_lr]\n",
    "\n",
    "    epochs=20 if FAST_RUN else 1000\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    hist = None\n",
    "    hist = classifier.fit(\n",
    "        train_generator, \n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=total_validate//batch_size,\n",
    "        steps_per_epoch=total_train//batch_size,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    finish = time.perf_counter()\n",
    "    print(\"Normal learning iteration {} : start-{} finish-{} training time-{}\".format(i,start, finish, finish-start))\n",
    "\n",
    "    gabor_history.append(hist)\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    plt.plot(hist.history['loss'], color='#785ef0')\n",
    "    plt.plot(hist.history['val_loss'], color='#dc267f')\n",
    "    plt.title('Model Loss Progress')\n",
    "    plt.ylabel('Binary Cross-Entropy Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training Set', 'Test Set'], loc='upper right')\n",
    "    plt.savefig('Iteration-{}-gabor-loss.png'.format(i), dpi=350, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    plt.plot(hist.history['accuracy'], color='#785ef0')\n",
    "    plt.plot(hist.history['val_accuracy'], color='#dc267f')\n",
    "    plt.title('Model Loss Progress')\n",
    "    plt.ylabel('Binary Cross-Entropy Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training Set', 'Test Set'], loc='upper right')\n",
    "    plt.savefig('Iteration-{}-gabor-gabor.png'.format(i), dpi=350, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    cnnl1 = classifier.layers[0].name   # get the name of the first conv layer\n",
    "    W = classifier.get_layer(name=cnnl1).get_weights()[0]   #get the filters\n",
    "    wshape = W.shape  #save the original shape\n",
    "\n",
    "    # this part will scale to [0, 1] for visualization purposes\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(W.reshape(-1,1))\n",
    "    W = scaler.transform(W.reshape(-1,1))\n",
    "    W = W.reshape(wshape)\n",
    "\n",
    "    # since there are 32 filters, we will display them 8x4\n",
    "    fig, axs = plt.subplots(8,4, figsize=(24,24))\n",
    "    fig.subplots_adjust(hspace = .25, wspace=.001)\n",
    "    axs = axs.ravel()\n",
    "    for i in range(W.shape[-1]):\n",
    "      # we reshape to a 3D (RGB) image shape and display\n",
    "      h = np.reshape(W[:,:,:,i], (15,15,3))\n",
    "      axs[i].imshow(h)\n",
    "      axs[i].set_title('Filter ' + str(i))    \n",
    "    plt.savefig('Iteration-{}-gabor-trainedFilters.png'.format(i), bbox_inches='tight', dpi=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
